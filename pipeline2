Here's a complete Named Entity Recognition (NER) pipeline using BERT (bert-base-ner) that includes data preprocessing, model training, evaluation, and saving predictions in CSV format. ğŸš€

ğŸ“Œ Steps in the Pipeline
1ï¸âƒ£ Load & Preprocess Data (Convert words & labels into BERT format)
2ï¸âƒ£ Tokenization (Using bert-base-ner)
3ï¸âƒ£ Prepare Dataset for Training
4ï¸âƒ£ Train the Model (Fine-tune bert-base-ner on NER dataset)
5ï¸âƒ£ Evaluate Performance (Precision, Recall, F1-Score)
6ï¸âƒ£ Predict on Train & Test Data (Save actual vs predicted tags in CSV)

ğŸ”¹ Step 1: Install & Import Dependencies
python
Copy
Edit
!pip install transformers datasets seqeval torch pandas numpy

import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from datasets import load_metric, Dataset, DatasetDict
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
ğŸ”¹ Step 2: Load & Preprocess Data
Your input data should be in CoNLL format (word, tag).
Example dataset:

mathematica
Copy
Edit
Apple B-ORG
is O
a O
company O
in O
USA B-LOC
. O
Load Data
python
Copy
Edit
def load_data(file_path):
    sentences = []
    sentence = []
    
    with open(file_path, "r") as f:
        for line in f:
            if line.strip():
                word, label = line.split()
                sentence.append((word, label))
            else:
                if sentence:
                    sentences.append(sentence)
                    sentence = []
    return sentences

train_sentences = load_data("train.txt")  # Change file path as needed
test_sentences = load_data("test.txt")
ğŸ”¹ Step 3: Convert Data for BERT
Extract Unique Labels
python
Copy
Edit
unique_labels = list(set(label for sent in train_sentences for _, label in sent))
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}
Tokenize & Align Labels
python
Copy
Edit
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")

def tokenize_and_align_labels(sentences):
    tokenized_inputs = tokenizer(
        [[word for word, label in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )
    
    labels = []
    for i, sentence in enumerate(sentences):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            else:
                label_ids.append(label2id[sentence[word_idx][1]])
        labels.append(label_ids)
    
    tokenized_inputs["labels"] = torch.tensor(labels)
    return tokenized_inputs
python
Copy
Edit
train_encodings = tokenize_and_align_labels(train_sentences)
test_encodings = tokenize_and_align_labels(test_sentences)
ğŸ”¹ Step 4: Prepare Dataset for Training
Convert to PyTorch dataset:

python
Copy
Edit
class NERDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
test_dataset = NERDataset(test_encodings)
ğŸ”¹ Step 5: Load Pretrained Model
python
Copy
Edit
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(unique_labels),
    id2label=id2label,
    label2id=label2id
)
ğŸ”¹ Step 6: Training Configuration
python
Copy
Edit
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)
ğŸ”¹ Step 7: Define Metrics
python
Copy
Edit
metric = load_metric("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)
    
    true_labels = [[id2label[label] for label in label_seq if label != -100] for label_seq in labels]
    pred_labels = [[id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100] 
                   for pred_seq, label_seq in zip(predictions, labels)]
    
    results = metric.compute(predictions=pred_labels, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
ğŸ”¹ Step 8: Train the Model
python
Copy
Edit
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()
ğŸ”¹ Step 9: Evaluate the Model
python
Copy
Edit
results = trainer.evaluate()
print("Evaluation Metrics:", results)
ğŸ”¹ Step 10: Save Predictions
Get Predicted Labels
python
Copy
Edit
def get_predictions(dataset):
    predictions, labels, _ = trainer.predict(dataset)
    predicted_ids = np.argmax(predictions, axis=2)

    true_labels = [[id2label[label] for label in label_seq if label != -100] for label_seq in labels]
    pred_labels = [[id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100] 
                   for pred_seq, label_seq in zip(predicted_ids, labels)]
    
    return true_labels, pred_labels
Convert to CSV
python
Copy
Edit
def save_predictions(sentences, true_labels, pred_labels, filename):
    data = []
    for sentence, actual, predicted in zip(sentences, true_labels, pred_labels):
        for word, act_tag, pred_tag in zip(sentence, actual, predicted):
            data.append([word[0], act_tag, pred_tag])

    df = pd.DataFrame(data, columns=["Word", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"Saved: {filename}")

# Save for Train & Test
train_true, train_pred = get_predictions(train_dataset)
test_true, test_pred = get_predictions(test_dataset)

save_predictions(train_sentences, train_true, train_pred, "train_predictions.csv")
save_predictions(test_sentences, test_true, test_pred, "test_predictions.csv")
ğŸ“Œ Final Output
1ï¸âƒ£ NER Model is trained using bert-base-ner
2ï¸âƒ£ Evaluation Metrics (Precision, Recall, F1-Score)
3ï¸âƒ£ Predictions saved as:
âœ… train_predictions.csv
âœ… test_predictions.csv

Word	Actual_Tag	Predicted_Tag
Apple	B-ORG	B-ORG
is	O	O
a	O	O
company	O	B-ORG
in	O	O
USA	B-LOC	B-LOC
ğŸ¯ Summary
âœ… Handles tokenization, subword alignment, and padding
âœ… Trains bert-base-ner for NER
âœ… Evaluates model performance
âœ… Saves actual vs predicted tags for both train & test data

Let me know if you need any modifications! ğŸš€
