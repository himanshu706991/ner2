Here’s the Python code for visualizing Banking data only, focusing on Net Loss Amount and Number of Events over time (monthly, quarterly, yearly).

📌 What This Code Does:
Filters data for Banking only.
Creates key visualizations:
Line Chart → Net Loss Trend Over Time
Box Plot → Distribution of Net Loss
Quarterly Bar Chart → Net Loss Comparison Over Quarters
Event Count Trend → Number of Events Over Time
Saves all plots into a PowerPoint file (Bank_Loss_Analysis.pptx).
📜 Full Code for Banking Data Analysis
python
Copy
Edit
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pptx import Presentation
from pptx.util import Inches
from io import BytesIO

# Load your Banking data (Replace with actual CSV or DataFrame)
data = {
    "Date": pd.date_range(start="2021-01-01", periods=48, freq="M"),
    "Net_Loss": [abs(x) for x in (1000000 * pd.np.random.randn(48)).tolist()],
    "Num_Events": [abs(int(x)) for x in (50 + 20 * pd.np.random.randn(48)).tolist()],
}

df = pd.DataFrame(data)
df["Date"] = pd.to_datetime(df["Date"])
df["Quarter"] = df["Date"].dt.to_period("Q")  # Convert to Quarterly Data

# Group Data for Quarterly Analysis
df_quarterly = df.groupby("Quarter")[["Net_Loss", "Num_Events"]].sum().reset_index()

# PowerPoint File
prs = Presentation()

def save_plot_to_ppt(title, fig, ppt):
    """ Saves a matplotlib figure to a PowerPoint slide. """
    slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # Blank slide
    title_box = slide.shapes.title
    title_box.text = title

    # Convert Matplotlib figure to PNG
    img_stream = BytesIO()
    fig.savefig(img_stream, format="png", bbox_inches="tight")
    img_stream.seek(0)

    # Insert image into slide
    slide.shapes.add_picture(img_stream, Inches(1), Inches(1), width=Inches(8))

    plt.close(fig)  # Close figure to free memory

# 📊 **1. Line Chart - Net Loss Over Time**
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df["Date"], df["Net_Loss"], marker='o', linestyle='-', color="b", label="Banking - Net Loss")
ax.set_xlabel("Date")
ax.set_ylabel("Net Loss Amount ($)")
ax.set_title("Net Loss Trend Over Time - Banking")
ax.legend()
ax.grid()
save_plot_to_ppt("Net Loss Trend Over Time - Banking", fig, prs)

# 📊 **2. Box Plot - Distribution of Net Loss**
fig, ax = plt.subplots(figsize=(8, 5))
sns.boxplot(y=df["Net_Loss"], ax=ax, color="blue")
ax.set_title("Distribution of Net Loss Amounts - Banking")
ax.set_ylabel("Net Loss Amount ($)")
ax.grid()
save_plot_to_ppt("Distribution of Net Loss - Banking", fig, prs)

# 📊 **3. Quarterly Bar Chart - Net Loss Comparison**
fig, ax = plt.subplots(figsize=(12, 6))
df_quarterly.plot(kind="bar", x="Quarter", y="Net_Loss", legend=False, ax=ax, color="blue")
ax.set_xlabel("Quarter (2021–2024)")
ax.set_ylabel("Total Net Loss ($)")
ax.set_title("Quarterly Net Loss - Banking")
plt.xticks(rotation=45)
save_plot_to_ppt("Quarterly Net Loss - Banking", fig, prs)

# 📊 **4. Line Chart - Number of Events Over Time**
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df["Date"], df["Num_Events"], marker='o', linestyle='-', color="r", label="Banking - Num of Events")
ax.set_xlabel("Date")
ax.set_ylabel("Number of Events")
ax.set_title("Event Count Trends Over Time - Banking")
ax.legend()
ax.grid()
save_plot_to_ppt("Event Count Trends Over Time - Banking", fig, prs)

# Save PowerPoint File
prs.save("Bank_Loss_Analysis.pptx")
print("PowerPoint file saved as Bank_Loss_Analysis.pptx")
📌 Expected Output:
✅ Line Chart → Banking Net Loss over time (2021–2024).
✅ Box Plot → Distribution of Net Loss for Banking (Detect Outliers).
✅ Quarterly Bar Chart → Comparison of Net Loss across Quarters.
✅ Event Count Trend → Number of Events over time.
✅ PowerPoint File Created → Bank_Loss_Analysis.pptx with all plots.

📌 How to Explain to the Client:
🔹 "Banking losses follow a stable upward trend with some peaks, showing higher volume and consistency."
🔹 "Quarterly analysis shows Banking losses are spread out across time, confirming stable loss accumulation."
🔹 "Number of events is consistently high, reinforcing why predictions work well for Banking."
🔹 "The Box Plot indicates extreme high-loss events but mostly within a reasonable range."


$$$$$$$$$$$$$$$$$$$$You're facing an issue where the Mean Absolute Percentage Error (MAPE) for the Property & Casualty (P&C) line of business is much higher compared to the Banking line of business. This discrepancy is likely due to data distribution and sample size differences between these business lines. Let’s break it down logically:

1. Why is MAPE higher for Property & Casualty?
MAPE is calculated as:

𝑀
𝐴
𝑃
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
∣
Actual
𝑖
−
Predicted
𝑖
Actual
𝑖
∣
×
100
MAPE= 
n
1
​
  
i=1
∑
n
​
  
​
  
Actual 
i
​
 
Actual 
i
​
 −Predicted 
i
​
 
​
  
​
 ×100
From your description, we can infer a few key reasons:

(A) Data Imbalance: Fewer Loss Events in P&C
You mentioned that the number of net loss events and amounts in P&C is significantly lower compared to Banking.
Monte Carlo simulations rely on sufficient historical data to generate realistic predictions. If P&C has fewer loss events, the model might struggle to estimate loss distributions accurately, leading to higher variance in predictions.
(B) Higher Relative Errors for Smaller Loss Amounts
If P&C has smaller absolute net loss values, any small error in prediction results in a large percentage error.
Example:
Banking: Actual = $10M, Predicted = $9.8M → MAPE = 2%
P&C: Actual = $1M, Predicted = $0.6M → MAPE = 40%
Since P&C has smaller losses, even minor prediction errors lead to disproportionately high MAPE.
(C) Monte Carlo Sensitivity to Low-Frequency Events
Monte Carlo relies on sampling from historical distributions. If P&C has fewer extreme losses, then extreme event estimations become less reliable.
This can lead to over- or under-estimation of risk, causing larger actual-predicted deviations.
(D) Different Volatility Patterns
Banking losses might have more consistent patterns, making prediction easier.
P&C losses could be more sporadic, rare, or highly volatile, making prediction more challenging.
2. How to Explain to the Client?
You can break it down like this:

"The high MAPE for P&C is due to the lower number of loss events and lower loss amounts compared to Banking."

Fewer data points lead to higher variance in predictions.
"Smaller absolute loss values in P&C mean that any deviation in predictions results in a large percentage error."

A $100K error in a $1M claim is 10% MAPE, while the same $100K error in a $10M claim is just 1% MAPE.
"Monte Carlo models perform better with sufficient data. Since Banking has more historical losses, the model predicts better compared to P&C, which has limited historical losses."

Fewer data points → Less reliable distribution estimation → Higher prediction errors.
"Property & Casualty losses may have more sporadic or extreme events, making prediction more challenging."

Unpredictable loss patterns lead to higher MAPE.
3. What Can You Do to Improve This?
If you need to improve the MAPE for P&C, here are some suggestions:

✅ Use alternative error metrics like RMSE or MAE, which are less sensitive to percentage-based variations.
✅ Group similar types of losses together to increase sample size for better estimation.
✅ Apply Bayesian updating or bootstrapping to better handle low-frequency loss events.
✅ Check for seasonality trends in P&C losses—Monte Carlo might need adjustments for such variations.

Let me know if you need any refinements in the explanation!







You said:
Is there any alternate method like Montek or low I can use for PNC separately same kind of personality like number of event and at loss with respect to these two can I do some other alternate method so that I can calculate the where
ChatGPT said:
Yes! Since Monte Carlo simulation is struggling with the low-frequency, high-variance nature of Property & Casualty (P&C) losses, you can try alternative methods that better handle sparse data and extreme event distributions. Here are a few effective approaches:

1. Extreme Value Theory (EVT)
Why?
P&C losses often follow heavy-tailed distributions, meaning that rare, extreme losses have a significant impact. EVT focuses on modeling the tail of the loss distribution, making it a strong alternative to Monte Carlo.

How?

Fit a Generalized Pareto Distribution (GPD) to the largest loss events (above a certain threshold).
Estimate parameters to model the probability of extreme losses occurring.
Use Peak Over Threshold (POT) method to simulate future losses.
Pros:
✔ Handles rare but severe loss events well.
✔ More accurate for estimating high-value claims.
✔ Works even with limited data, as it focuses only on extreme values.

Cons:
✘ Requires choosing a threshold, which can be tricky.
✘ More complex than Monte Carlo.

2. Bayesian Hierarchical Modeling
Why?
If P&C has fewer loss events, Bayesian methods can borrow information from similar categories to make better predictions.

How?

Define a prior distribution based on historical loss patterns.
Use Bayesian inference to update estimates as new loss data comes in.
Works well for low-data scenarios, as it smooths estimates using prior knowledge.
Pros:
✔ Handles small datasets well.
✔ Can incorporate expert/business knowledge into predictions.
✔ Provides uncertainty quantification (posterior distributions).

Cons:
✘ Computationally expensive.
✘ Requires domain expertise for setting priors.

3. Bootstrapping
Why?
If P&C loss data is limited, resampling techniques like bootstrapping can help estimate variability without assuming a fixed probability distribution.

How?

Randomly resample the available loss data (with replacement).
Create multiple synthetic datasets and compute mean/percentiles for estimation.
Use these distributions to model future losses.
Pros:
✔ Works well with small datasets.
✔ No need for strong distributional assumptions.
✔ Easy to implement.

Cons:
✘ Assumes past loss patterns will continue, which may not always hold.
✘ Computationally intensive if dataset is very large.

4. Copula-Based Simulation
Why?
If net loss amounts and number of events are correlated, copulas can model their joint distribution better than Monte Carlo.

How?

Use Copula functions (Gaussian, Clayton, or Gumbel) to capture dependencies between event frequency and loss amount.
Generate synthetic loss-event pairs based on these learned relationships.
Pros:
✔ Captures non-linear dependencies between variables.
✔ More flexible than traditional Monte Carlo.

Cons:
✘ Requires expertise in selecting the right copula function.
✘ Computationally demanding.

5. Generalized Linear Models (GLM) for Loss Distribution
Why?
GLMs are commonly used in insurance risk modeling because they allow modeling both frequency (number of events) and severity (loss amount) separately.

How?

Model event count using Poisson or Negative Binomial regression.
Model loss amount using Gamma, Weibull, or Lognormal distribution.
Use these models to generate synthetic losses.
Pros:
✔ Commonly used in insurance industry.
✔ Can incorporate external risk factors.
✔ Provides interpretable results.

Cons:
✘ Assumes specific distributions (needs careful selection).
✘ May not work well for extreme losses unless combined with EVT.

Recommendation for P&C
Given the challenges in P&C (low frequency, extreme losses), I suggest a hybrid approach:

1️⃣ Use Extreme Value Theory (EVT) to model rare, extreme losses.
2️⃣ Use Bootstrapping if you have very few data points.
3️⃣ If you need a structured model, try GLMs for loss frequency/severity.
4️⃣ If dependency between loss amount and event count is strong, Copula-Based Simulation can help.

Let me know if you want sample code or a deeper dive into any of these methods!

You said:
With respect to our discussion can you create some bullet points for that I can set into the PPT I need to explain to the entire story to the client I need to prepare one two two three pH PPT on this
ChatGPT said:
Sure! Below are key bullet points that you can use for your PPT slides to explain the issue and alternative solutions clearly to your client.

Slide 1: Understanding the Issue – High MAPE in Property & Casualty (P&C)
🔹 Objective: Evaluating wear value using Monte Carlo for different lines of business (Banking vs. P&C).
🔹 Observation: MAPE (Mean Absolute Percentage Error) is high for P&C but reasonable for Banking.
🔹 Why?

Data imbalance: Banking has higher loss events and amounts than P&C.
Small loss values in P&C: Even small prediction errors cause high percentage errors in MAPE.
Low-frequency events: Fewer extreme events make Monte Carlo less reliable for P&C.
Higher variability in P&C losses compared to Banking.
Slide 2: Why Monte Carlo Fails for P&C?
🔹 Monte Carlo relies on sufficient historical loss data to simulate realistic future losses.
🔹 Challenges with P&C data:

Fewer historical events → Poor estimation of distributions.
Higher uncertainty in event occurrences and loss amounts.
Extreme losses are rare, making Monte Carlo less effective in capturing tail risks.
Slide 3: Alternative Approaches for P&C Wear Value Estimation
🔹 Given P&C’s characteristics, alternative methods can improve prediction accuracy:

1️⃣ Extreme Value Theory (EVT) – Models rare, extreme losses effectively.

Uses Generalized Pareto Distribution (GPD) to estimate high-loss probabilities.
Ideal for heavy-tailed loss distributions (large but rare claims).
2️⃣ Bayesian Hierarchical Modeling – Helps with low-frequency events.

Uses prior knowledge and updates estimates with new data.
Effective for small datasets where Monte Carlo fails.
3️⃣ Bootstrapping – Resampling technique for better distribution estimation.

Works well when historical loss data is limited.
Reduces variance in predictions.
4️⃣ Copula-Based Simulation – Captures relationships between loss amount & event frequency.

Models dependencies better than Monte Carlo.
Helps in correlated event-loss scenarios.
5️⃣ Generalized Linear Models (GLM) – Common in insurance modeling.

Models event frequency (Poisson/Negative Binomial).
Models loss severity (Gamma/Lognormal).
Can be combined with EVT for extreme losses.
Slide 4: Recommendations & Next Steps
✅ Short-Term Fix

Replace MAPE with MAE or RMSE for better accuracy evaluation.
Increase P&C sample size by combining similar risk categories.
✅ Long-Term Solution

Use EVT for extreme loss events in P&C.
Try GLM or Bayesian methods for better frequency-severity modeling.
Consider bootstrapping to compensate for low data volume.
Explore Copula models if dependencies between loss frequency and severity exist.
🔹 Outcome: Improved prediction accuracy, better risk estimation, and clearer insights for business decisions.

This structure gives a clear, data-driven story for the client. You can add visuals like:

A comparison table showing why Monte Carlo fails for P&C.
A chart showing how EVT captures extreme losses better.
A graph of loss frequency/severity distributions before and after model improvements.
Let me know if you need refinements or slides in PowerPoint format!

You said:
My client is continuously asking when Bank having high number of net loss high number of number of event but that may a value is very very less but in property and casualty you have less number of net loss even frequency also less but still you are getting very high value for the map that point is not at all understanding I need to give the strong evidence so that he can understand properly
ChatGPT said:
Your client is struggling to understand why the MAPE for Banking is low while it's high for P&C, despite P&C having fewer events and lower net loss amounts. To convince them, you need to provide a strong mathematical and statistical explanation with clear evidence. Here’s how you can frame it:

Key Reasons for High MAPE in Property & Casualty (P&C)
1️⃣ MAPE is Sensitive to Small Actual Values
Formula:

𝑀
𝐴
𝑃
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
∣
Actual
𝑖
−
Predicted
𝑖
Actual
𝑖
∣
×
100
MAPE= 
n
1
​
  
i=1
∑
n
​
  
​
  
Actual 
i
​
 
Actual 
i
​
 −Predicted 
i
​
 
​
  
​
 ×100
In Banking, net loss amounts are large → The denominator (Actual) is high → MAPE remains low even for moderate prediction errors.
In P&C, net loss amounts are smaller → The denominator is small → Even small absolute errors result in large percentage errors.
Example:
Business Line	Actual Loss	Predicted Loss	Absolute Error	MAPE (%)
Banking	$10M	$9.5M	$0.5M	5%
P&C	$1M	$0.6M	$0.4M	40%
🔹 Even though the absolute error ($0.5M vs. $0.4M) is similar, the percentage error is much higher for P&C because of the smaller actual values.

2️⃣ Low Sample Size in P&C Increases Variability
Banking has a high number of events → More data points → More stable prediction models → Lower MAPE.
P&C has fewer events → Less data to train models → Predictions fluctuate more → Higher MAPE.
📌 Think of it like rolling a dice:

If you roll it 10,000 times (like Banking losses), the average outcome is stable (~3.5).
If you roll it 10 times (like P&C losses), you might get highly varying results (sometimes too low or too high).
3️⃣ High Variability in P&C Losses Affects Predictions
Banking losses are often normally distributed, making them easier to predict.
P&C losses have extreme variations (a mix of small claims and rare catastrophic events).
Monte Carlo simulations work better with large, stable datasets like Banking but struggle with volatile, low-frequency data like P&C.
📊 Supporting Visual:

Show a histogram or boxplot comparing Banking vs. P&C loss distributions.
Banking should have a narrower distribution, while P&C should show a wider range with more extreme values.
Final Explanation to the Client
🔹 "The reason MAPE is high for P&C is due to the mathematical nature of the formula, not because the model is fundamentally incorrect."
🔹 "When actual loss values are small, any small error leads to a high percentage error, which inflates MAPE."
🔹 "Additionally, P&C has fewer data points and higher variability, making predictions inherently less stable."
🔹 "A better metric for evaluation would be Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE), as they do not suffer from percentage-based distortions."

📌 Next Steps (for client confidence):
✅ Show distribution comparisons (boxplots/histograms) to illustrate Banking vs. P&C loss behavior.





$$$$$$$$$$$$$$$$$$$$$$$
To visually demonstrate the distribution difference between Banking and Property & Casualty (P&C) over time (monthly, quarterly, or yearly), you can use the following visualization techniques:

1️⃣ Visualizing Net Loss Amount & Number of Events (Time Series & Distribution Plots)
A. Line Chart: Trend Over Time (Monthly, Quarterly, Yearly)
📊 Use Case:

Helps show the trend over time (2021–2024) for both Banking & P&C.
Clearly highlights that Banking losses/events are consistently higher than P&C.
📌 How to Plot (Matplotlib/Seaborn/Pandas)?

X-axis: Time (Month/Quarter/Year)
Y-axis: Net Loss Amount or Number of Events
Separate lines for Banking vs. P&C
python
Copy
Edit
import pandas as pd
import matplotlib.pyplot as plt

# Assuming 'df' contains 'Date', 'Net_Loss', 'Num_Events', and 'LOB' (Line of Business)
df['Date'] = pd.to_datetime(df['Date'])
df['Quarter'] = df['Date'].dt.to_period('Q')  # Extract quarter

# Grouping data quarterly
df_quarterly = df.groupby(['Quarter', 'LOB'])[['Net_Loss', 'Num_Events']].sum().reset_index()

# Plot
plt.figure(figsize=(12, 6))
for lob in df_quarterly['LOB'].unique():
    subset = df_quarterly[df_quarterly['LOB'] == lob]
    plt.plot(subset['Quarter'].astype(str), subset['Net_Loss'], label=f'{lob} - Net Loss')

plt.xticks(rotation=45)
plt.xlabel("Quarter (2021–2024)")
plt.ylabel("Total Net Loss ($)")
plt.title("Quarterly Net Loss Trends for Banking vs. P&C")
plt.legend()
plt.grid()
plt.show()
🔹 Expected Outcome:

The Banking curve will be significantly higher than P&C.
P&C will show a lower, fluctuating trend.
Clear evidence that Banking has higher frequency & magnitude of losses.
B. Box Plot: Distribution Comparison
📊 Use Case:

Compare the spread & variance of losses/events between Banking and P&C.
Banking should show a wider spread (higher losses), while P&C should have a tighter distribution.
📌 How to Plot?

X-axis: Line of Business (Banking vs. P&C)
Y-axis: Net Loss Amount or Number of Events
python
Copy
Edit
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.boxplot(x=df['LOB'], y=df['Net_Loss'])
plt.title("Distribution of Net Loss Amounts (Banking vs. P&C)")
plt.ylabel("Net Loss Amount ($)")
plt.xlabel("Line of Business")
plt.grid()
plt.show()
🔹 Expected Outcome:

Banking will have a wider boxplot (higher loss range).
P&C will have lower median and smaller spread.
Shows Banking has more extreme high-loss events than P&C.
C. Bar Chart: Total Loss & Event Count by Quarter
📊 Use Case:

Compares total losses & number of events per quarter for both LOBs.
Stacked or grouped bars to show Banking vs. P&C trends.
📌 How to Plot?

python
Copy
Edit
df_quarterly.pivot(index="Quarter", columns="LOB", values="Net_Loss").plot(kind="bar", figsize=(12, 6))
plt.xlabel("Quarter (2021–2024)")
plt.ylabel("Total Net Loss ($)")
plt.title("Quarterly Comparison of Net Loss: Banking vs. P&C")
plt.xticks(rotation=45)
plt.legend(title="Line of Business")
plt.show()
🔹 Expected Outcome:

Banking bars will consistently be higher.
P&C bars will be shorter, indicating lower losses.
Gap between bars highlights the difference in loss magnitude.
2️⃣ Final Slide Explanation for the Client
✅ "From the time series trend, it’s evident that Banking consistently has more loss events and higher loss amounts compared to P&C."
✅ "Box plots confirm that Banking losses have a higher range and extreme values, whereas P&C losses are smaller and less frequent."
✅ "Quarterly bar charts show that Banking dominates in both event frequency and loss magnitude across all periods."
✅ "This distribution difference is why Monte Carlo predictions behave differently for Banking vs. P&C, leading to higher MAPE for P&C."

Would you like a PowerPoint-ready version of these insights? Let me know!







You said:
Yeah sure and consider like I have three column net loss date and number of event with respect to both kind of property in casualty as well as for the back both different line of business with respect to that create a code
ChatGPT said:
Here’s the full Python code to generate key visualizations comparing Banking vs. Property & Casualty (P&C) based on Net Loss, Date, and Number of Events. This code will help you create a PowerPoint presentation-ready analysis with clear insights.

🔹 What This Code Does:
Reads & Prepares Data (Assumes columns: Date, Net_Loss, Num_Events, LOB)
Generates Key Visualizations:
Line Plot (Trend of Net Loss over Time for both LOBs)
Box Plot (Distribution of Net Loss for Banking vs. P&C)
Quarterly Bar Chart (Comparison of Total Net Loss across time)
Event Count Trend (Number of Events over Time)
Creates a PowerPoint-ready file (LOB_Analysis.pptx) with visualizations.
📌 Full Code
python
Copy
Edit
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pptx import Presentation
from pptx.util import Inches
from io import BytesIO

# Sample Data (Replace with actual CSV file)
data = {
    "Date": pd.date_range(start="2021-01-01", periods=48, freq="M").tolist() * 2,
    "Net_Loss": [abs(x) for x in (1000000 * pd.np.random.randn(48)).tolist()] + 
                [abs(x) for x in (300000 * pd.np.random.randn(48)).tolist()],
    "Num_Events": [abs(int(x)) for x in (50 + 20 * pd.np.random.randn(48)).tolist()] + 
                  [abs(int(x)) for x in (10 + 5 * pd.np.random.randn(48)).tolist()],
    "LOB": ["Banking"] * 48 + ["P&C"] * 48
}

df = pd.DataFrame(data)
df["Date"] = pd.to_datetime(df["Date"])
df["Quarter"] = df["Date"].dt.to_period("Q")  # Convert to Quarterly Data

# Group Data for Visualization
df_quarterly = df.groupby(["Quarter", "LOB"])[["Net_Loss", "Num_Events"]].sum().reset_index()

# PowerPoint File
prs = Presentation()

def save_plot_to_ppt(title, fig, ppt):
    """ Saves a matplotlib figure to a PowerPoint slide. """
    slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # Blank slide
    title_box = slide.shapes.title
    title_box.text = title

    # Convert Matplotlib figure to PNG
    img_stream = BytesIO()
    fig.savefig(img_stream, format="png", bbox_inches="tight")
    img_stream.seek(0)

    # Insert image into slide
    slide.shapes.add_picture(img_stream, Inches(1), Inches(1), width=Inches(8))

    plt.close(fig)  # Close figure to free memory

# 📊 **1. Line Plot - Net Loss Over Time**
fig, ax = plt.subplots(figsize=(12, 6))
for lob in df["LOB"].unique():
    subset = df[df["LOB"] == lob]
    ax.plot(subset["Date"], subset["Net_Loss"], label=f"{lob} - Net Loss")

ax.set_xlabel("Date")
ax.set_ylabel("Net Loss Amount ($)")
ax.set_title("Net Loss Trends Over Time (Banking vs. P&C)")
ax.legend()
ax.grid()
save_plot_to_ppt("Net Loss Trend Over Time", fig, prs)

# 📊 **2. Box Plot - Distribution of Net Loss**
fig, ax = plt.subplots(figsize=(8, 5))
sns.boxplot(x=df["LOB"], y=df["Net_Loss"], ax=ax)
ax.set_title("Distribution of Net Loss Amounts (Banking vs. P&C)")
ax.set_ylabel("Net Loss Amount ($)")
ax.set_xlabel("Line of Business")
ax.grid()
save_plot_to_ppt("Distribution of Net Loss (Banking vs. P&C)", fig, prs)

# 📊 **3. Quarterly Bar Chart - Net Loss Comparison**
fig, ax = plt.subplots(figsize=(12, 6))
df_quarterly.pivot(index="Quarter", columns="LOB", values="Net_Loss").plot(kind="bar", ax=ax)
ax.set_xlabel("Quarter (2021–2024)")
ax.set_ylabel("Total Net Loss ($)")
ax.set_title("Quarterly Net Loss Comparison: Banking vs. P&C")
ax.legend(title="Line of Business")
plt.xticks(rotation=45)
save_plot_to_ppt("Quarterly Net Loss Comparison", fig, prs)

# 📊 **4. Line Chart - Number of Events Over Time**
fig, ax = plt.subplots(figsize=(12, 6))
for lob in df["LOB"].unique():
    subset = df[df["LOB"] == lob]
    ax.plot(subset["Date"], subset["Num_Events"], label=f"{lob} - Num of Events")

ax.set_xlabel("Date")
ax.set_ylabel("Number of Events")
ax.set_title("Event Count Trends Over Time (Banking vs. P&C)")
ax.legend()
ax.grid()
save_plot_to_ppt("Event Count Trends Over Time", fig, prs)

# Save PowerPoint File
prs.save("LOB_Analysis.pptx")
print("PowerPoint file saved as LOB_Analysis.pptx")
📌 What This Code Does:
✔ Loads data with Date, Net_Loss, Num_Events, and LOB (Banking or P&C).
✔ Prepares Quarterly Aggregates for trend analysis.
✔ Creates four key plots:

Line Chart (Net Loss over time)
Box Plot (Distribution of Net Loss)
Bar Chart (Quarterly Net Loss Comparison)
Line Chart (Number of Events over time)
✔ Saves all plots into a PowerPoint file (LOB_Analysis.pptx).
📊 Expected Outcome:
Line Chart - Net Loss Over Time:

Banking will have a higher curve showing higher loss trends.
P&C will have a lower, fluctuating trend with some spikes.
Box Plot - Distribution of Net Loss:

Banking’s boxplot will be wider, showing higher and more varied losses.
P&C’s boxplot will be narrower, showing smaller and less frequent losses.
Quarterly Bar Chart - Net Loss Comparison:

Banking bars will be consistently higher.
P&C bars will be shorter, showing a significant difference.
Event Count Trends:

Banking will have more frequent and higher event counts.
P&C will have fewer events, making predictions more volatile.
📌 How to Explain to the Client:
✅ "Banking losses are consistently higher, whereas P&C losses are lower but have extreme fluctuations."
✅ "Banking’s distribution is wider, meaning the model gets more stable predictions."
✅ "P&C has low-frequency, high-variability losses, making percentage-based errors (MAPE) larger."
✅ "This is why Monte Carlo struggles more with P&C, leading to higher MAPE values."

📩 Final Deliverables
✔ PowerPoint file LOB_Analysis.pptx with all key visualizations.
✔ Matplotlib & Seaborn-based Python code for insights.

Would you like me to customize the PowerPoint file further (titles, additional slides, annotations)? Let me know!
✅ Provide alternative metrics (MAE/RMSE) to evaluate model performance fairly.
✅ If required, use Extreme Value Theory (EVT) for rare high-impact events in P&C.

Would you like me to create a PowerPoint slide deck with these key points? Let me know how detailed you need it!
