 Issue: KeyError: word_ids Not Found in Dataset
The error means that word_ids() is not stored inside your train_dataset and test_dataset.

ğŸ”¹ Step 1: Debug and Verify if word_ids() Exists
Before fixing, print a sample from train_dataset to check if word_ids is available.

python
Copy
Edit
print("Sample from train_dataset:", train_dataset[0])  # ğŸ” Check if "word_ids" exists
print("Sample from test_dataset:", test_dataset[0])
ğŸ”¹ Expected Output (If word_ids exists)

python
Copy
Edit
{'input_ids': tensor([...]), 'attention_mask': tensor([...]), 'labels': tensor([...]), 'word_ids': [0, 1, 1, 2, 3, None]}
ğŸ”¹ If word_ids is missing (KeyError appears), apply the fixes below.

âœ… Fix 1: Explicitly Store word_ids() in Tokenized Dataset
Modify the tokenization function to store word_ids() manually.

python
Copy
Edit
import torch

def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,  # âœ… Ensures correct token alignment
        return_tensors="pt"
    )

    word_ids_list = []  # âœ… Explicitly store `word_ids`
    aligned_labels = []

    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # âœ… Get word-to-token mapping
        if word_ids is None:
            raise ValueError("âŒ Error: `word_ids()` is missing! Ensure `use_fast=True` in tokenizer.")

        word_ids_list.append(word_ids)

        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    tokenized_inputs["word_ids"] = word_ids_list  # âœ… Store `word_ids`
    return tokenized_inputs
âœ… Fixes:

Ensures word_ids is explicitly stored in dataset.
Prevents KeyError: word_ids in later steps.
âœ… Fix 2: Recreate the train_dataset and test_dataset
After fixing the tokenization, recreate your dataset:

python
Copy
Edit
train_encodings = tokenize_and_align_labels(train_sentences, train_labels, tokenizer)
test_encodings = tokenize_and_align_labels(test_sentences, test_labels, tokenizer)

class NERDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
test_dataset = NERDataset(test_encodings)
âœ… Fixes:

Creates a fresh dataset with word_ids stored properly.
Ensures the dataset has input_ids, labels, attention_mask, and word_ids.
âœ… Fix 3: Modify get_predictions() to Check for word_ids()
Modify the prediction function to check if word_ids exists before using it.

python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer, id2label):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    output_list = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        # âœ… Ensure `word_ids` exists before accessing it
        if "word_ids" not in dataset[i]:
            raise ValueError("âŒ Error: `word_ids` not found in dataset. Ensure tokenization stored them correctly.")

        word_ids = dataset[i]["word_ids"]
        previous_word_idx = None

        sentence_tokens = []
        sentence_true_labels = []
        sentence_pred_labels = []

        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            sentence_tokens.append(tokens[j])
            sentence_true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))
            sentence_pred_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))

            previous_word_idx = word_idx

        output_list.append([sentence_tokens, sentence_true_labels, sentence_pred_labels])

    return output_list
âœ… Fixes:

Ensures word_ids is present before accessing it.
Prevents crashes due to missing mappings.
âœ… Final Steps: Check and Generate Predictions
1ï¸âƒ£ Verify word_ids Exists Before Running Predictions
python
Copy
Edit
print("Sample from train_dataset:", train_dataset[0])  # ğŸ” Verify word_ids exists
print("Sample from test_dataset:", test_dataset[0])
If word_ids is missing, re-run Fix 1 and Fix 2 before proceeding.

2ï¸âƒ£ Generate Train & Test Predictions
python
Copy
Edit
train_predictions_list = get_predictions(trainer, train_dataset, tokenizer, id2label)
test_predictions_list = get_predictions(trainer, test_dataset, tokenizer, id2label)

print("Train Example:", train_predictions_list[:3])
print("\nTest Example:", test_predictions_list[:3])
ğŸ“‚ Expected Output Format (List)
python
Copy
Edit
[
    [
        ['Elon', 'Musk', 'founded', 'SpaceX', 'in', '2002', '.'],  # Tokens
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O'],  # Actual Tags
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O']   # Predicted Tags
    ],
    [
        ['Google', 'is', 'a', 'tech', 'company', '.'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O']
    ]
]
âœ… Each sentence is stored as a nested list [Tokens, Actual Tags, Predicted Tags].

ğŸš€ Summary
âœ… Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
âœ… Explicitly stores word_ids() in dataset (Fix 1).
âœ… Recreates dataset with word_ids stored (Fix 2).
âœ… Ensures word_ids exists before predictions (Fix 3).

ğŸ” Next Steps
Try these fixes and print dataset samples to check if word_ids is present.
If the error persists, run print(train_dataset[0]) and verify that word_ids exists.
ğŸš€ Let me know if the issue is resolved! ğŸ”¥

You said:
see i have two input file which has sentenceid,token,tag and other file has sentienceid,description and bert-ner model that all i have
ChatGPT said:
ğŸš€ Complete Pipeline: Named Entity Recognition (NER) with bert-base-ner
You have:
âœ… Two input files:
1ï¸âƒ£ tokens.csv â†’ (Sentence_ID, Token, Tag)
2ï¸âƒ£ sentences.csv â†’ (Sentence_ID, Description)
âœ… Pretrained Model: bert-base-ner

ğŸ”¹ Goal: Train bert-base-ner on your dataset, predict NER labels, and save actual vs. predicted tags in CSV.

ğŸ“Œ Steps in the Pipeline
1ï¸âƒ£ Load Data (tokens.csv + sentences.csv)
2ï¸âƒ£ Preprocess Data (Align tokens & labels for BERT)
3ï¸âƒ£ Train bert-base-ner (Fine-tune on your dataset)
4ï¸âƒ£ Evaluate Model (Compute precision, recall, F1-score)
5ï¸âƒ£ Save Results: (Token, Token ID, Actual Tag, Predicted Tag)

ğŸ”¹ Step 1: Load Data
python
Copy
Edit
import pandas as pd

# Load files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge to align tokens with full sentences
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")

# Fill missing tags with 'O' (Outside of Named Entity)
merged_df["Tag"] = merged_df["Tag"].fillna("O")

# Display dataset sample
print(merged_df.head())
âœ… Ensures missing tags (NaN) are replaced with "O".

ğŸ”¹ Step 2: Prepare Data for BERT
Extract Sentences & Labels
python
Copy
Edit
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()
âœ… Each sentence is grouped together with its tokens & labels.

ğŸ”¹ Step 3: Tokenization & Word Alignment
python
Copy
Edit
import torch
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Word alignment
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Convert labels into numerical IDs
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

# Tokenize dataset
tokenized_inputs = tokenize_and_align_labels(sentences, labels)
âœ… Handles subword tokenization & aligns labels correctly.

ğŸ”¹ Step 4: Prepare Dataset for Training
python
Copy
Edit
from torch.utils.data import Dataset

class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

dataset = NERDataset(tokenized_inputs)
âœ… Creates a PyTorch dataset for training.

ğŸ”¹ Step 5: Train BERT Model
python
Copy
Edit
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments

model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

trainer.train()
âœ… Fine-tunes bert-base-ner on your dataset.

ğŸ”¹ Step 6: Evaluate Model Performance
python
Copy
Edit
import numpy as np
from datasets import load_metric

metric = load_metric("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = [[id2label[label] for label in label_seq if label != -100] for label_seq in labels]
    pred_labels = [[id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100] 
                   for pred_seq, label_seq in zip(predictions, labels)]

    results = metric.compute(predictions=pred_labels, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

trainer.evaluate()
âœ… Displays precision, recall, and F1-score for named entity classification.

ğŸ”¹ Step 7: Save Predictions in CSV
python
Copy
Edit
def save_predictions(dataset, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)
    labels = predictions_output.label_ids

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        for j, token in enumerate(tokens):
            actual_tag = id2label.get(labels[i][j], "O")
            predicted_tag = id2label.get(predictions[i][j], "O")

            data.append([token, input_ids[j], actual_tag, predicted_tag])

    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")

save_predictions(dataset, "ner_predictions.csv")
âœ… Creates ner_predictions.csv with Actual vs. Predicted NER tags.

ğŸ“‚ Expected CSV Output
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
ğŸš€ Summary
âœ… Loads data from tokens.csv & sentences.csv
âœ… Tokenizes & aligns labels correctly
âœ… Trains bert-base-ner on custom dataset
âœ… Evaluates & generates performance metrics
âœ… Saves final predictions in ner_predictions.csv

ğŸš€ Let me know if you need modifications or explanations! ğŸ”¥
