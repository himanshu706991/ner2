Got it! You need a complete NER pipeline where:
1Ô∏è‚É£ Train & Save the NER Model using bert-base-ner
2Ô∏è‚É£ Load Model for Predictions on new test data
3Ô∏è‚É£ Generate Confusion Matrix comparing actual vs. predicted tags

üîπ Step 1: Train & Save the NER Model
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

# Tokenization & Alignment Function
def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize dataset
train_encodings = tokenize_and_align_labels(sentences, labels)

# Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

dataset = NERDataset(train_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

# Train & Save Model
trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("‚úÖ Model Trained & Saved as 'ner_model'")
‚úÖ Trains and Saves bert-base-ner Model

üîπ Step 2: Load Model & Predict on New Data
python
Copy
Edit
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Test Data
df_test = pd.read_csv("test_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
test_sentences = df_test.groupby("Sentence_ID")["Token"].apply(list).tolist()
test_labels = df_test.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []
    
    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(test_sentences)

# Save Predictions
df_test["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_test.to_csv("test_predictions.csv", index=False)
print("‚úÖ Predictions Saved in 'test_predictions.csv'")
‚úÖ Loads the model and predicts labels for fresh test data

üîπ Step 3: Generate Confusion Matrix
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Flatten actual & predicted labels
actual_labels = df_test["Actual_Tag"].tolist()
predicted_labels = df_test["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
‚úÖ Generates confusion matrix & classification report

üìÇ Expected Final Outputs
1Ô∏è‚É£ ner_model/ ‚Üí Model files (pytorch_model.bin, config.json, etc.)
2Ô∏è‚É£ test_predictions.csv ‚Üí Token-wise predictions
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3Ô∏è‚É£ Confusion Matrix & Report
‚úÖ Visualizes misclassified tags
‚úÖ Precision, Recall, and F1-score per tag

üöÄ Summary
‚úî Trains and Saves bert-base-ner Model
‚úî Loads Model & Makes Predictions on New Data
‚úî Generates Confusion Matrix & Performance Report

Now you can pass fresh test files, predict token-level NER labels, and analyze errors! üöÄüî• Let me know if you need modifications!

################%&$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4

suppose i want tpass training data as innput ofr validation that code plz

ChatGPT said:
üöÄ Use Training Data for Validation in NER Model
If you want to use the training data as validation data, modify the Trainer setup to include both train and validation datasets.

üîπ Step 1: Train-Test Split for Validation
Modify the dataset split so that a part of the training data is used as validation instead of a separate test set.

python
Copy
Edit
from sklearn.model_selection import train_test_split

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# ‚úÖ Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

print(f"Training Samples: {len(train_sentences)}, Validation Samples: {len(val_sentences)}")
‚úÖ Now val_sentences will be used for validation instead of a separate test set.

üîπ Step 2: Tokenization & Alignment for Validation
python
Copy
Edit
import torch
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)
‚úÖ Now val_encodings contains the validation dataset.

üîπ Step 3: Create Train & Validation Datasets
python
Copy
Edit
from torch.utils.data import Dataset

class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)
‚úÖ Creates Train & Validation datasets separately.

üîπ Step 4: Train Model with Validation Data
python
Copy
Edit
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # ‚úÖ Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # ‚úÖ Saves the best model based on validation
    load_best_model_at_end=True,  # ‚úÖ Keeps the best model
    eval_steps=50  # ‚úÖ Evaluate every 50 steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # ‚úÖ Uses training data as validation
    tokenizer=tokenizer
)

trainer.train()
‚úÖ Now training data is used for validation!
‚úÖ Best model is saved based on validation performance!

üîπ Step 5: Evaluate on Validation Set
python
Copy
Edit
trainer.evaluate()
‚úÖ Prints validation accuracy, precision, recall, and F1-score

üöÄ Summary
‚úî Train-Test Split modified to Train-Validation Split
‚úî Validation Data used in Trainer for Evaluation
‚úî Best Model Saved Based on Validation Performance

Now your training data is also used for validation instead of a separate test set. üöÄüî• Let me know if you need any modifications!

You said:
for validation also consider input data crete seprate scripts for predction that have all preprocessing
ChatGPT said:
üöÄ Full Pipeline: Train, Validate, and Predict with a Separate Script
This pipeline will:
‚úÖ Train bert-base-ner and Save the Model
‚úÖ Use Training Data as Validation (Train-Validation Split)
‚úÖ Create a Separate Prediction Script
‚úÖ Ensure Full Preprocessing in Prediction Script

üìå Part 1: Train & Save the Model (Train + Validation)
üîπ train_ner.py (Train & Save Model)
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# ‚úÖ Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)

# Create Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # ‚úÖ Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # ‚úÖ Saves best model based on validation
    load_best_model_at_end=True,  # ‚úÖ Keeps the best model
    eval_steps=50
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # ‚úÖ Uses training data for validation
    tokenizer=tokenizer
)

trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("‚úÖ Model Trained & Saved as 'ner_model'")
‚úÖ Trains and Saves the Model with Validation

üìå Part 2: Separate Script for Prediction
üîπ predict_ner.py (Load Model & Predict on New Data)
python
Copy
Edit
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model & Tokenizer
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Validation Data
df_val = pd.read_csv("val_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
val_sentences = df_val.groupby("Sentence_ID")["Token"].apply(list).tolist()
actual_labels = df_val.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []

    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(val_sentences)

# Save Predictions
df_val["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_val.to_csv("val_predictions.csv", index=False)
print("‚úÖ Predictions Saved in 'val_predictions.csv'")
‚úÖ Loads the model and predicts labels for validation data

üìå Part 3: Confusion Matrix for Validation
üîπ evaluate_ner.py (Generate Confusion Matrix)
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Load Predicted Results
df_val = pd.read_csv("val_predictions.csv")

# Flatten actual & predicted labels
actual_labels = df_val["Actual_Tag"].tolist()
predicted_labels = df_val["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
‚úÖ Generates confusion matrix & classification report for validation data

üìÇ Expected Final Outputs
1Ô∏è‚É£ ner_model/ ‚Üí Saved model
2Ô∏è‚É£ val_predictions.csv ‚Üí Actual vs. Predicted Tags
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3Ô∏è‚É£ Confusion Matrix & Report
‚úÖ Visualizes misclassified tags
‚úÖ Precision, Recall, and F1-score per tag

üöÄ Summary
‚úî Separate Scripts for Training, Prediction, and Evaluation
‚úî Validation Data Used for Performance Testing
‚úî Confusion Matrix & Performance Report Generated

üöÄ Now you can train, validate, and evaluate the model separately! üî• Let me know if you need modifications!
