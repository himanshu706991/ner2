Issue: "Pre-tokenized encode input must be a union of pretokenized input sequences"
This error occurs when is_split_into_words=True is used incorrectly. The tokenizer expects a list of words (tokens), but it might be getting a string instead.

âœ… Fix: Ensure Sentences Are Properly Tokenized Before Tokenization
Before passing sentences to the tokenizer, make sure they are lists of words.

python
Copy
Edit
# âœ… Ensure each sentence is a list of words, not a string
train_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in train_sentences]
val_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in val_sentences]
âœ… This ensures every sentence is a list of tokens (words), not a single string.

âœ… Fix: Modify Tokenization Function
ğŸ”¹ Corrected tokenize_and_align_labels() Function
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels):
    # âœ… Ensure sentences are in the correct format (list of words)
    if isinstance(sentences[0], str):  
        sentences = [sentence.split() for sentence in sentences]  

    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,  # âœ… Ensure proper word alignment
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # âœ… Get word IDs
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
âœ… Fixes:

Ensures sentences are passed as lists of words before tokenization
Prevents pre-tokenized encode input must be pretokenized error
âœ… Fix: Debug Input Before Tokenization
Before calling the tokenize_and_align_labels() function, print sample sentences.

python
Copy
Edit
print("Sample Train Sentence:", train_sentences[0])  # Should be a list of words, NOT a string
print("Sample Validation Sentence:", val_sentences[0])  # Should be a list of words
ğŸ”¹ If the output is a string, apply .split() as shown in Fix 1.

âœ… Final Steps: Tokenize Train & Validation Data
python
Copy
Edit
# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)
âœ… Now train_encodings and val_encodings are correctly formatted

ğŸš€ Summary
âœ… Ensures sentences are tokenized into word lists before passing to tokenizer()
âœ… Fixes pre-tokenized encode input must be a union of pretokenized sequences error
âœ… Prevents issues with incorrect is_split_into_words=True usage

Try these fixes and let me know if it works! ğŸš€ğŸ”¥









##############
 Issue: "Pre-tokenized encode input must be a union of pre-tokenized input sequences"
This error happens when the tokenizer expects pre-tokenized input but receives an incorrectly formatted string or list.

âœ… Fix: Ensure Sentences Are Passed as Lists of Words
Modify the prediction function to properly split and pass tokens to tokenizer().

ğŸ”¹ Correct Prediction Function
python
Copy
Edit
def predict_ner(sentences):
    predictions_list = []

    for sentence in sentences:
        # âœ… Ensure sentence is a list of words
        if isinstance(sentence, str):
            sentence = sentence.split()  

        # âœ… Tokenization with word alignment
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)

        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list
âœ… Fix: Verify Input Before Tokenization
Before calling tokenizer(), check the format of sentences.

python
Copy
Edit
print("Sample Sentence:", val_sentences[0])  # Should be a list of words, NOT a string
If it prints a string, convert it into a list of words:

python
Copy
Edit
val_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in val_sentences]
âœ… Ensures every input sentence is a properly tokenized list.

ğŸš€ Final Fix Summary
âœ… Ensures sentences are tokenized as lists before passing to tokenizer()
âœ… Prevents pre-tokenized encode input must be a union of pre-tokenized error
âœ… Handles subwords correctly for NER alignment

Try this and let me know if it works! ğŸš€ğŸ”¥


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Got it! You need a complete NER pipeline where:
1ï¸âƒ£ Train & Save the NER Model using bert-base-ner
2ï¸âƒ£ Load Model for Predictions on new test data
3ï¸âƒ£ Generate Confusion Matrix comparing actual vs. predicted tags

ğŸ”¹ Step 1: Train & Save the NER Model
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

# Tokenization & Alignment Function
def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize dataset
train_encodings = tokenize_and_align_labels(sentences, labels)

# Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

dataset = NERDataset(train_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

# Train & Save Model
trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("âœ… Model Trained & Saved as 'ner_model'")
âœ… Trains and Saves bert-base-ner Model

ğŸ”¹ Step 2: Load Model & Predict on New Data
python
Copy
Edit
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Test Data
df_test = pd.read_csv("test_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
test_sentences = df_test.groupby("Sentence_ID")["Token"].apply(list).tolist()
test_labels = df_test.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []
    
    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(test_sentences)

# Save Predictions
df_test["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_test.to_csv("test_predictions.csv", index=False)
print("âœ… Predictions Saved in 'test_predictions.csv'")
âœ… Loads the model and predicts labels for fresh test data

ğŸ”¹ Step 3: Generate Confusion Matrix
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Flatten actual & predicted labels
actual_labels = df_test["Actual_Tag"].tolist()
predicted_labels = df_test["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
âœ… Generates confusion matrix & classification report

ğŸ“‚ Expected Final Outputs
1ï¸âƒ£ ner_model/ â†’ Model files (pytorch_model.bin, config.json, etc.)
2ï¸âƒ£ test_predictions.csv â†’ Token-wise predictions
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3ï¸âƒ£ Confusion Matrix & Report
âœ… Visualizes misclassified tags
âœ… Precision, Recall, and F1-score per tag

ğŸš€ Summary
âœ” Trains and Saves bert-base-ner Model
âœ” Loads Model & Makes Predictions on New Data
âœ” Generates Confusion Matrix & Performance Report

Now you can pass fresh test files, predict token-level NER labels, and analyze errors! ğŸš€ğŸ”¥ Let me know if you need modifications!

################%&$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4

suppose i want tpass training data as innput ofr validation that code plz

ChatGPT said:
ğŸš€ Use Training Data for Validation in NER Model
If you want to use the training data as validation data, modify the Trainer setup to include both train and validation datasets.

ğŸ”¹ Step 1: Train-Test Split for Validation
Modify the dataset split so that a part of the training data is used as validation instead of a separate test set.

python
Copy
Edit
from sklearn.model_selection import train_test_split

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# âœ… Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

print(f"Training Samples: {len(train_sentences)}, Validation Samples: {len(val_sentences)}")
âœ… Now val_sentences will be used for validation instead of a separate test set.

ğŸ”¹ Step 2: Tokenization & Alignment for Validation
python
Copy
Edit
import torch
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)
âœ… Now val_encodings contains the validation dataset.

ğŸ”¹ Step 3: Create Train & Validation Datasets
python
Copy
Edit
from torch.utils.data import Dataset

class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)
âœ… Creates Train & Validation datasets separately.

ğŸ”¹ Step 4: Train Model with Validation Data
python
Copy
Edit
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # âœ… Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # âœ… Saves the best model based on validation
    load_best_model_at_end=True,  # âœ… Keeps the best model
    eval_steps=50  # âœ… Evaluate every 50 steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # âœ… Uses training data as validation
    tokenizer=tokenizer
)

trainer.train()
âœ… Now training data is used for validation!
âœ… Best model is saved based on validation performance!

ğŸ”¹ Step 5: Evaluate on Validation Set
python
Copy
Edit
trainer.evaluate()
âœ… Prints validation accuracy, precision, recall, and F1-score

ğŸš€ Summary
âœ” Train-Test Split modified to Train-Validation Split
âœ” Validation Data used in Trainer for Evaluation
âœ” Best Model Saved Based on Validation Performance

Now your training data is also used for validation instead of a separate test set. ğŸš€ğŸ”¥ Let me know if you need any modifications!

You said:
for validation also consider input data crete seprate scripts for predction that have all preprocessing
ChatGPT said:
ğŸš€ Full Pipeline: Train, Validate, and Predict with a Separate Script
This pipeline will:
âœ… Train bert-base-ner and Save the Model
âœ… Use Training Data as Validation (Train-Validation Split)
âœ… Create a Separate Prediction Script
âœ… Ensure Full Preprocessing in Prediction Script

ğŸ“Œ Part 1: Train & Save the Model (Train + Validation)
ğŸ”¹ train_ner.py (Train & Save Model)
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# âœ… Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)

# Create Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # âœ… Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # âœ… Saves best model based on validation
    load_best_model_at_end=True,  # âœ… Keeps the best model
    eval_steps=50
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # âœ… Uses training data for validation
    tokenizer=tokenizer
)

trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("âœ… Model Trained & Saved as 'ner_model'")
âœ… Trains and Saves the Model with Validation

ğŸ“Œ Part 2: Separate Script for Prediction
ğŸ”¹ predict_ner.py (Load Model & Predict on New Data)
python
Copy
Edit
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model & Tokenizer
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Validation Data
df_val = pd.read_csv("val_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
val_sentences = df_val.groupby("Sentence_ID")["Token"].apply(list).tolist()
actual_labels = df_val.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []

    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(val_sentences)

# Save Predictions
df_val["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_val.to_csv("val_predictions.csv", index=False)
print("âœ… Predictions Saved in 'val_predictions.csv'")
âœ… Loads the model and predicts labels for validation data

ğŸ“Œ Part 3: Confusion Matrix for Validation
ğŸ”¹ evaluate_ner.py (Generate Confusion Matrix)
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Load Predicted Results
df_val = pd.read_csv("val_predictions.csv")

# Flatten actual & predicted labels
actual_labels = df_val["Actual_Tag"].tolist()
predicted_labels = df_val["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
âœ… Generates confusion matrix & classification report for validation data

ğŸ“‚ Expected Final Outputs
1ï¸âƒ£ ner_model/ â†’ Saved model
2ï¸âƒ£ val_predictions.csv â†’ Actual vs. Predicted Tags
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3ï¸âƒ£ Confusion Matrix & Report
âœ… Visualizes misclassified tags
âœ… Precision, Recall, and F1-score per tag

ğŸš€ Summary
âœ” Separate Scripts for Training, Prediction, and Evaluation
âœ” Validation Data Used for Performance Testing
âœ” Confusion Matrix & Performance Report Generated

ğŸš€ Now you can train, validate, and evaluate the model separately! ğŸ”¥ Let me know if you need modifications!
