Issue: "Pre-tokenized encode input must be a union of pretokenized input sequences"
This error occurs when is_split_into_words=True is used incorrectly. The tokenizer expects a list of words (tokens), but it might be getting a string instead.

‚úÖ Fix: Ensure Sentences Are Properly Tokenized Before Tokenization
Before passing sentences to the tokenizer, make sure they are lists of words.

python
Copy
Edit
# ‚úÖ Ensure each sentence is a list of words, not a string
train_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in train_sentences]
val_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in val_sentences]
‚úÖ This ensures every sentence is a list of tokens (words), not a single string.

‚úÖ Fix: Modify Tokenization Function
üîπ Corrected tokenize_and_align_labels() Function
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels):
    # ‚úÖ Ensure sentences are in the correct format (list of words)
    if isinstance(sentences[0], str):  
        sentences = [sentence.split() for sentence in sentences]  

    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,  # ‚úÖ Ensure proper word alignment
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # ‚úÖ Get word IDs
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
‚úÖ Fixes:

Ensures sentences are passed as lists of words before tokenization
Prevents pre-tokenized encode input must be pretokenized error
‚úÖ Fix: Debug Input Before Tokenization
Before calling the tokenize_and_align_labels() function, print sample sentences.

python
Copy
Edit
print("Sample Train Sentence:", train_sentences[0])  # Should be a list of words, NOT a string
print("Sample Validation Sentence:", val_sentences[0])  # Should be a list of words
üîπ If the output is a string, apply .split() as shown in Fix 1.

‚úÖ Final Steps: Tokenize Train & Validation Data
python
Copy
Edit
# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)
‚úÖ Now train_encodings and val_encodings are correctly formatted

üöÄ Summary
‚úÖ Ensures sentences are tokenized into word lists before passing to tokenizer()
‚úÖ Fixes pre-tokenized encode input must be a union of pretokenized sequences error
‚úÖ Prevents issues with incorrect is_split_into_words=True usage

Try these fixes and let me know if it works! üöÄüî•









##############
 Issue: "Pre-tokenized encode input must be a union of pre-tokenized input sequences"
This error happens when the tokenizer expects pre-tokenized input but receives an incorrectly formatted string or list.

‚úÖ Fix: Ensure Sentences Are Passed as Lists of Words
Modify the prediction function to properly split and pass tokens to tokenizer().

üîπ Correct Prediction Function
python
Copy
Edit
def predict_ner(sentences):
    predictions_list = []

    for sentence in sentences:
        # ‚úÖ Ensure sentence is a list of words
        if isinstance(sentence, str):
            sentence = sentence.split()  

        # ‚úÖ Tokenization with word alignment
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)

        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list
‚úÖ Fix: Verify Input Before Tokenization
Before calling tokenizer(), check the format of sentences.

python
Copy
Edit
print("Sample Sentence:", val_sentences[0])  # Should be a list of words, NOT a string
If it prints a string, convert it into a list of words:

python
Copy
Edit
val_sentences = [sentence.split() if isinstance(sentence, str) else sentence for sentence in val_sentences]
‚úÖ Ensures every input sentence is a properly tokenized list.

üöÄ Final Fix Summary
‚úÖ Ensures sentences are tokenized as lists before passing to tokenizer()
‚úÖ Prevents pre-tokenized encode input must be a union of pre-tokenized error
‚úÖ Handles subwords correctly for NER alignment

Try this and let me know if it works! üöÄüî•


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Got it! You need a complete NER pipeline where:
1Ô∏è‚É£ Train & Save the NER Model using bert-base-ner
2Ô∏è‚É£ Load Model for Predictions on new test data
3Ô∏è‚É£ Generate Confusion Matrix comparing actual vs. predicted tags

üîπ Step 1: Train & Save the NER Model
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

# Tokenization & Alignment Function
def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize dataset
train_encodings = tokenize_and_align_labels(sentences, labels)

# Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

dataset = NERDataset(train_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

# Train & Save Model
trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("‚úÖ Model Trained & Saved as 'ner_model'")
‚úÖ Trains and Saves bert-base-ner Model

üîπ Step 2: Load Model & Predict on New Data
python
Copy
Edit
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Test Data
df_test = pd.read_csv("test_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
test_sentences = df_test.groupby("Sentence_ID")["Token"].apply(list).tolist()
test_labels = df_test.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []
    
    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(test_sentences)

# Save Predictions
df_test["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_test.to_csv("test_predictions.csv", index=False)
print("‚úÖ Predictions Saved in 'test_predictions.csv'")
‚úÖ Loads the model and predicts labels for fresh test data

üîπ Step 3: Generate Confusion Matrix
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Flatten actual & predicted labels
actual_labels = df_test["Actual_Tag"].tolist()
predicted_labels = df_test["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
‚úÖ Generates confusion matrix & classification report

üìÇ Expected Final Outputs
1Ô∏è‚É£ ner_model/ ‚Üí Model files (pytorch_model.bin, config.json, etc.)
2Ô∏è‚É£ test_predictions.csv ‚Üí Token-wise predictions
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3Ô∏è‚É£ Confusion Matrix & Report
‚úÖ Visualizes misclassified tags
‚úÖ Precision, Recall, and F1-score per tag

üöÄ Summary
‚úî Trains and Saves bert-base-ner Model
‚úî Loads Model & Makes Predictions on New Data
‚úî Generates Confusion Matrix & Performance Report

Now you can pass fresh test files, predict token-level NER labels, and analyze errors! üöÄüî• Let me know if you need modifications!

################%&$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4

suppose i want tpass training data as innput ofr validation that code plz

ChatGPT said:
üöÄ Use Training Data for Validation in NER Model
If you want to use the training data as validation data, modify the Trainer setup to include both train and validation datasets.

üîπ Step 1: Train-Test Split for Validation
Modify the dataset split so that a part of the training data is used as validation instead of a separate test set.

python
Copy
Edit
from sklearn.model_selection import train_test_split

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# ‚úÖ Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

print(f"Training Samples: {len(train_sentences)}, Validation Samples: {len(val_sentences)}")
‚úÖ Now val_sentences will be used for validation instead of a separate test set.

üîπ Step 2: Tokenization & Alignment for Validation
python
Copy
Edit
import torch
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)
‚úÖ Now val_encodings contains the validation dataset.

üîπ Step 3: Create Train & Validation Datasets
python
Copy
Edit
from torch.utils.data import Dataset

class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)
‚úÖ Creates Train & Validation datasets separately.

üîπ Step 4: Train Model with Validation Data
python
Copy
Edit
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # ‚úÖ Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # ‚úÖ Saves the best model based on validation
    load_best_model_at_end=True,  # ‚úÖ Keeps the best model
    eval_steps=50  # ‚úÖ Evaluate every 50 steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # ‚úÖ Uses training data as validation
    tokenizer=tokenizer
)

trainer.train()
‚úÖ Now training data is used for validation!
‚úÖ Best model is saved based on validation performance!

üîπ Step 5: Evaluate on Validation Set
python
Copy
Edit
trainer.evaluate()
‚úÖ Prints validation accuracy, precision, recall, and F1-score

üöÄ Summary
‚úî Train-Test Split modified to Train-Validation Split
‚úî Validation Data used in Trainer for Evaluation
‚úî Best Model Saved Based on Validation Performance

Now your training data is also used for validation instead of a separate test set. üöÄüî• Let me know if you need any modifications!

You said:
for validation also consider input data crete seprate scripts for predction that have all preprocessing
ChatGPT said:
üöÄ Full Pipeline: Train, Validate, and Predict with a Separate Script
This pipeline will:
‚úÖ Train bert-base-ner and Save the Model
‚úÖ Use Training Data as Validation (Train-Validation Split)
‚úÖ Create a Separate Prediction Script
‚úÖ Ensure Full Preprocessing in Prediction Script

üìå Part 1: Train & Save the Model (Train + Validation)
üîπ train_ner.py (Train & Save Model)
python
Copy
Edit
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Load input files
df_tokens = pd.read_csv("tokens.csv")  # Sentence_ID, Token, Tag
df_sentences = pd.read_csv("sentences.csv")  # Sentence_ID, Description

# Merge sentence descriptions with tokens
merged_df = df_tokens.merge(df_sentences, on="Sentence_ID", how="left")
merged_df["Tag"] = merged_df["Tag"].fillna("O")  # Replace NaN tags with "O"

# Group tokens & labels by sentence
sentences = merged_df.groupby("Sentence_ID")["Token"].apply(list).tolist()
labels = merged_df.groupby("Sentence_ID")["Tag"].apply(list).tolist()

# ‚úÖ Train-Validation Split (80% Train, 20% Validation)
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=42
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)

# Convert labels to numerical format
unique_labels = list(set(merged_df["Tag"].dropna().unique()))
label2id = {label: i for i, label in enumerate(["O"] + unique_labels)}
id2label = {i: label for label, i in label2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        sentences, 
        is_split_into_words=True,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs

# Tokenize datasets
train_encodings = tokenize_and_align_labels(train_sentences, train_labels)
val_encodings = tokenize_and_align_labels(val_sentences, val_labels)

# Create Dataset Class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)

# Load Model
model = AutoModelForTokenClassification.from_pretrained(
    "dslim/bert-base-NER",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# Training Arguments with Validation
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",  # ‚úÖ Enables evaluation after every epoch
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",  # ‚úÖ Saves best model based on validation
    load_best_model_at_end=True,  # ‚úÖ Keeps the best model
    eval_steps=50
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # ‚úÖ Uses training data for validation
    tokenizer=tokenizer
)

trainer.train()
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
print("‚úÖ Model Trained & Saved as 'ner_model'")
‚úÖ Trains and Saves the Model with Validation

üìå Part 2: Separate Script for Prediction
üîπ predict_ner.py (Load Model & Predict on New Data)
python
Copy
Edit
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Model & Tokenizer
model = AutoModelForTokenClassification.from_pretrained("ner_model")
tokenizer = AutoTokenizer.from_pretrained("ner_model")

# Load New Validation Data
df_val = pd.read_csv("val_tokens.csv")  # Sentence_ID, Token, Actual_Tag

# Group tokens & labels by sentence
val_sentences = df_val.groupby("Sentence_ID")["Token"].apply(list).tolist()
actual_labels = df_val.groupby("Sentence_ID")["Actual_Tag"].apply(list).tolist()

# Tokenization & Prediction Function
def predict_ner(sentences):
    predictions_list = []

    for sentence in sentences:
        inputs = tokenizer(sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
        predicted_labels = [id2label[p] for p in predictions]

        predictions_list.append(predicted_labels)
    
    return predictions_list

# Get Predictions
predicted_labels = predict_ner(val_sentences)

# Save Predictions
df_val["Predicted_Tag"] = [tag for pred in predicted_labels for tag in pred]
df_val.to_csv("val_predictions.csv", index=False)
print("‚úÖ Predictions Saved in 'val_predictions.csv'")
‚úÖ Loads the model and predicts labels for validation data

üìå Part 3: Confusion Matrix for Validation
üîπ evaluate_ner.py (Generate Confusion Matrix)
python
Copy
Edit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# Load Predicted Results
df_val = pd.read_csv("val_predictions.csv")

# Flatten actual & predicted labels
actual_labels = df_val["Actual_Tag"].tolist()
predicted_labels = df_val["Predicted_Tag"].tolist()

# Filter only valid labels (exclude "O" if needed)
valid_labels = sorted(set(actual_labels + predicted_labels))
conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=valid_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=valid_labels, yticklabels=valid_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("NER Confusion Matrix")
plt.show()

# Print Classification Report
print("NER Model Performance:")
print(classification_report(actual_labels, predicted_labels))
‚úÖ Generates confusion matrix & classification report for validation data

üìÇ Expected Final Outputs
1Ô∏è‚É£ ner_model/ ‚Üí Saved model
2Ô∏è‚É£ val_predictions.csv ‚Üí Actual vs. Predicted Tags
Sentence_ID	Token	Actual_Tag	Predicted_Tag
1	Elon	B-PER	B-PER
1	Musk	I-PER	I-PER
1	founded	O	O
1	SpaceX	B-ORG	B-ORG
3Ô∏è‚É£ Confusion Matrix & Report
‚úÖ Visualizes misclassified tags
‚úÖ Precision, Recall, and F1-score per tag

üöÄ Summary
‚úî Separate Scripts for Training, Prediction, and Evaluation
‚úî Validation Data Used for Performance Testing
‚úî Confusion Matrix & Performance Report Generated

üöÄ Now you can train, validate, and evaluate the model separately! üî• Let me know if you need modifications!
