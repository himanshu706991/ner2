
 Issue: KeyError: word_ids in Dataset
This error occurs because word_ids is not available in your dataset. This happens when:
1ï¸âƒ£ The tokenizer is not a "Fast" tokenizer (use_fast=True is required).
2ï¸âƒ£ The dataset format is incompatible with word_ids().
3ï¸âƒ£ The dataset structure is not aligned properly after tokenization.

âœ… Fix: Ensure Fast Tokenizer is Used
Modify your tokenizer initialization to force using a Fast Tokenizer:

python
Copy
Edit
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)  # âœ… Ensure Fast Tokenizer
ğŸ”¹ Why?
Only Fast Tokenizers support .word_ids(), which is required for word alignment.

âœ… Fix: Correct Tokenization to Include word_ids()
Modify the tokenization function to correctly map words and labels.

python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],  # Extract words
        truncation=True,
        padding=True,
        is_split_into_words=True,  # âœ… Needed for `word_ids()`
        return_tensors="pt"
    )

    # Ensure word_ids() is available
    if not hasattr(tokenized_inputs, "word_ids"):
        raise ValueError("Error: The tokenizer does not support `word_ids`. Use a `FastTokenizer`.")

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # âœ… Map tokens to words
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding and special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
âœ… Fixes:

Ensures word_ids() is available before using it.
Uses batch_index=i correctly (avoids missing word mappings).
âœ… Fix: Modify Prediction Function to Ensure word_ids() Exists
If your dataset still doesnâ€™t have word_ids(), update the prediction function to verify its presence.

python
Copy
Edit
import numpy as np
import pandas as pd

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        # Ensure word_ids exists
        word_ids = dataset[i].get("word_ids", None)
        if word_ids is None:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")  # Get actual tag
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")  # Get predicted tag

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")
âœ… Fixes:

Checks if word_ids is available before accessing it.
Ensures proper token-label alignment before saving CSV.
âœ… Final Steps: Generate CSV for Train & Test
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
ğŸ“‚ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
ğŸš€ Summary
âœ… Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
âœ… Checks if word_ids exists before calling it.
âœ… Handles subword alignment correctly (avoids extra/missing tokens).
âœ… Saves results in a structured CSV format for Train & Test data.

Try these fixes and let me know if the error is resolved! ğŸš€ğŸ”¥

You said:
i am using fast tokenizer
ChatGPT said:
Since youâ€™re using a Fast Tokenizer, but still getting KeyError: word_ids, letâ€™s systematically debug and fix it.

ğŸ” Issue Breakdown
Even with a Fast Tokenizer, word_ids() might still fail due to:
1ï¸âƒ£ Incorrect Dataset Handling â€“ Some datasets donâ€™t retain word_ids() correctly.
2ï¸âƒ£ Misalignment After Tokenization â€“ Word IDs might not be properly mapped.
3ï¸âƒ£ Dataset Format Differences â€“ The dataset might not store word_ids() correctly.

âœ… Fix 1: Check if word_ids() is Available
Before calling word_ids(), print debug info to confirm if it exists.

python
Copy
Edit
# Tokenize a sample sentence and check word_ids
sample_sentence = ["Elon", "Musk", "founded", "SpaceX", "in", "2002", "."]
tokenized_sample = tokenizer(sample_sentence, is_split_into_words=True, return_tensors="pt")

print("Tokenized Input IDs:", tokenized_sample["input_ids"])
print("Word IDs:", tokenized_sample.word_ids() if hasattr(tokenized_sample, "word_ids") else "âŒ Not Available")
ğŸ”¹ If word_ids() prints None, the tokenizer isnâ€™t returning word mappings.
ğŸ”¹ If âŒ Not Available appears, your dataset might not be storing word_ids().

âœ… Fix 2: Ensure word_ids() is Stored in Dataset
Modify dataset creation to explicitly store word_ids() in each sample.

ğŸ”¹ Correct Tokenization & Dataset Creation
python
Copy
Edit
import torch

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    aligned_labels = []
    word_ids_list = []  # âœ… Store word IDs separately
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        word_ids_list.append(word_ids)  # âœ… Store word IDs

        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    tokenized_inputs["word_ids"] = word_ids_list  # âœ… Ensure word_ids are stored
    return tokenized_inputs
âœ… Fixes:

Ensures word_ids() is stored in the dataset.
Prevents KeyError: word_ids by keeping mappings available.
âœ… Fix 3: Modify Prediction Function to Access word_ids Safely
Modify your prediction function to safely access word_ids().

python
Copy
Edit
import numpy as np
import pandas as pd

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)
    labels = predictions_output.label_ids

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        # âœ… Safely check for `word_ids`
        if "word_ids" not in dataset[i]:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        word_ids = dataset[i]["word_ids"]  # âœ… Access stored word IDs
        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")
âœ… Fixes:

Ensures word_ids exists in dataset before accessing it.
Prevents KeyError: word_ids when processing predictions.
âœ… Final Steps: Generate CSV for Train & Test
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
ğŸ“‚ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
ğŸš€ Summary
âœ… Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
âœ… Stores word_ids inside dataset explicitly to avoid missing mappings.
âœ… Safely accesses word_ids during predictions to prevent crashes.
âœ… Saves results in structured CSV format for Train & Test data.

ğŸ” Next Steps
Try these fixes and check if word_ids() is properly stored in your dataset.
Print debug information (print(dataset_train[0])) to verify data structure.
Run the CSV export function to check if final outputs are correct.
ğŸš€ Let me know if the issue is resolved! ğŸ”¥


################################################################$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4444444
$$$Keys error: word_ids in both cases i am getting error
ChatGPT said:
ğŸ” Issue: KeyError: word_ids in Predictions
The error occurs because word_ids() is not available in some tokenizers (especially when using a tokenizer that doesnâ€™t support word alignment).

âœ… Fix 1: Ensure Correct Tokenizer Usage
Make sure your tokenizer supports word alignment by using BertTokenizerFast.

python
Copy
Edit
from transformers import AutoTokenizer

# Ensure using a fast tokenizer that supports word_ids()
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)
âœ… Why?
Only Fast Tokenizers (like BertTokenizerFast) support word_ids() for word alignment.

âœ… Fix 2: Use .word_ids(batch_index=i) Properly
Modify the tokenization & prediction functions without assuming word_ids() is always present.

ğŸ”¹ Correct Tokenization Function
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    if not hasattr(tokenized_inputs, "word_ids"):
        raise ValueError("Error: The tokenizer does not support `word_ids`. Use a `FastTokenizer`.")

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # âœ… Ensures correct batch handling
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
âœ… Fixes:

Ensures word_ids() exists before calling it.
Uses batch_index=i correctly (avoids missing word mappings).
âœ… Fix 3: Handle word_ids() in Prediction Function
Modify the prediction function to ensure word alignment.

ğŸ”¹ Correct Prediction Function
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    output_list = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        
        # Ensure word_ids exists
        if not hasattr(dataset[i], "word_ids"):
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")
        
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        sentence_tokens = []
        sentence_true_labels = []
        sentence_pred_labels = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            sentence_tokens.append(tokens[j])
            sentence_true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))
            sentence_pred_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))

            previous_word_idx = word_idx

        output_list.append([sentence_tokens, sentence_true_labels, sentence_pred_labels])

    return output_list
âœ… Fixes:

Checks if word_ids exists before using it.
Ensures subwords donâ€™t get duplicate labels.
Properly maps predictions back to words.
âœ… Final Steps
1ï¸âƒ£ Get Train & Test Predictions
python
Copy
Edit
train_predictions_list = get_predictions(trainer, dataset_train, tokenizer)
test_predictions_list = get_predictions(trainer, dataset_test, tokenizer)
2ï¸âƒ£ Print & Verify Outputs
python
Copy
Edit
print("Train Example:", train_predictions_list[:3])
print("\nTest Example:", test_predictions_list[:3])
3ï¸âƒ£ Save to JSON (Optional)
python
Copy
Edit
import json

with open("train_predictions.json", "w") as f:
    json.dump(train_predictions_list, f, indent=4)

with open("test_predictions.json", "w") as f:
    json.dump(test_predictions_list, f, indent=4)

print("âœ… Saved train_predictions.json and test_predictions.json")
ğŸ¯ Summary
âœ… Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
âœ… Checks if word_ids exists before calling it.
âœ… Handles subword alignment correctly (avoids extra/missing tokens).
âœ… Saves results in a clean list format or JSON file.

ğŸš€ Try these fixes & let me know if the error is resolved! ğŸ”¥





You said:
ok this is for list formate , i also need in csv, tokenid,predtag,actual tag
ChatGPT said:
âœ… Saving Predictions in CSV Format (Token, Token ID, Actual Tag, Predicted Tag)
Since you need Token, Token ID, Actual Tag, and Predicted Tag stored in a CSV file, we'll modify our function accordingly.

ğŸ”¹ Step 1: Modify Prediction Function for CSV
python
Copy
Edit
import pandas as pd
import numpy as np

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)
        word_ids = dataset[i].get("word_ids", None)  # Get word mappings if available

        if word_ids is None:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")  # Get actual tag
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")  # Get predicted tag

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")
ğŸ”¹ Step 2: Generate CSV Files for Train & Test Data
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
ğŸ“‚ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
ğŸš€ Summary
âœ… Fixes missing/misaligned tokens issue
âœ… Ensures Token, Token ID, Actual Tag, and Predicted Tag are aligned
âœ… Saves results in a structured CSV format for Train & Test data

Now you can compare actual vs predicted tags in CSV easily! ğŸ¯ Let me know if you need further refinements. ğŸš€ğŸ”¥





###############################################################################################3
Yes! You can store the output in a list format, where each entry contains:
1ï¸âƒ£ Tokens (Words)
2ï¸âƒ£ Actual Tags (Ground Truth)
3ï¸âƒ£ Predicted Tags (Model Output)

âœ… Storing Output in a List Format
Modify the function to return the output as a list of lists instead of a CSV file:

python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    output_list = []  # Store final output in list format

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()  # Convert token IDs to tokens
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        sentence_tokens = []
        sentence_true_labels = []
        sentence_pred_labels = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding
            
            sentence_tokens.append(tokens[j])  # Store token
            sentence_true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))  # Actual tag
            sentence_pred_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))  # Predicted tag

            previous_word_idx = word_idx

        # Store as a list: [tokens_list, actual_tags_list, predicted_tags_list]
        output_list.append([sentence_tokens, sentence_true_labels, sentence_pred_labels])

    return output_list
âœ… Get Train & Test Predictions in List Format
python
Copy
Edit
train_predictions_list = get_predictions(trainer, dataset_train, tokenizer)
test_predictions_list = get_predictions(trainer, dataset_test, tokenizer)

# Example: Check the first few sentences
print("Train Example:", train_predictions_list[:3])
print("\nTest Example:", test_predictions_list[:3])
ğŸ“Œ Example Output (List Format)
python
Copy
Edit
[
    [
        ['Elon', 'Musk', 'founded', 'SpaceX', 'in', '2002', '.'],  # Tokens
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O'],  # Actual Tags
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O']   # Predicted Tags
    ],
    [
        ['Google', 'is', 'a', 'tech', 'company', '.'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O']
    ]
]
âœ… Each sentence is stored as a nested list with [Tokens, Actual Tags, Predicted Tags].

ğŸš€ Benefits of List Format
âœ… Easy to process in Python (can iterate over sentences easily).
âœ… No formatting issues like CSV (ensures all words align correctly).
âœ… Can be saved in JSON for better readability.
âœ… Save Output to JSON (Optional)
If you want to save this structured format to a file:

python
Copy
Edit
import json

with open("train_predictions.json", "w") as f:
    json.dump(train_predictions_list, f, indent=4)

with open("test_predictions.json", "w") as f:
    json.dump(test_predictions_list, f, indent=4)

print("âœ… Saved train_predictions.json and test_predictions.json")
This will store the structured format in JSON.

ğŸ¯ Summary
You can now store output in a structured list format:
[Tokens, Actual Tags, Predicted Tags]
No subword issues, padding issues, or misaligned tokens.
Optionally save the results in JSON for easy visualization.
Try this and let me know if you need further modifications! ğŸš€ğŸ”¥


######

ğŸ” Issue: Tokens Not Aligning Properly in Final CSV
If your final train/test CSV files have errors such as:
1ï¸âƒ£ Subwords instead of full words (e.g., "comp", "##uter" instead of "computer")
2ï¸âƒ£ Mismatched token counts (Extra/missing words)
3ï¸âƒ£ Too many arguments error

âœ… Fix: Properly Align Tokens, Actual Tags, and Predicted Tags
Modify your tokenization & alignment process to handle subwords correctly.

ğŸ”¹ Step 1: Adjust Tokenization & Alignment
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id[sentence_labels[word_idx]])  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subword tokens
                
            previous_word_idx = word_idx
        
        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
âœ… This ensures:

Only the first subword gets a label
Subwords (##word) donâ€™t get duplicate labels
Special tokens ([CLS], [SEP], [PAD]) are ignored
ğŸ”¹ Step 2: Generate Predictions & Align Them with Tokens
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Actual label indices

    true_labels_list = []
    predicted_labels_list = []
    tokens_list = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()  # Convert token IDs to tokens
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        true_labels = []
        predicted_labels = []
        aligned_tokens = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding
            
            aligned_tokens.append(tokens[j])  # Get full words
            true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))  # Assign correct actual label
            predicted_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))  # Assign predicted label

            previous_word_idx = word_idx

        tokens_list.append(aligned_tokens)
        true_labels_list.append(true_labels)
        predicted_labels_list.append(predicted_labels)

    return tokens_list, true_labels_list, predicted_labels_list
âœ… This ensures:

Subword tokens donâ€™t appear separately
Actual & predicted labels match words correctly
ğŸ”¹ Step 3: Save Train & Test Predictions to CSV
python
Copy
Edit
import pandas as pd

def save_predictions(tokens, true_labels, pred_labels, filename):
    flat_tokens = [word for sentence in tokens for word in sentence]
    flat_actual_labels = [label for sentence in true_labels for label in sentence]
    flat_predicted_labels = [label for sentence in pred_labels for label in sentence]

    df = pd.DataFrame({"Token": flat_tokens, "Actual_Tag": flat_actual_labels, "Predicted_Tag": flat_predicted_labels})
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")

# ğŸ”¹ Get predictions for Train & Test
train_tokens, train_true_labels, train_pred_labels = get_predictions(trainer, dataset_train, tokenizer)
test_tokens, test_true_labels, test_pred_labels = get_predictions(trainer, dataset_test, tokenizer)

# ğŸ”¹ Save to CSV
save_predictions(train_tokens, train_true_labels, train_pred_labels, "final_train_predictions.csv")
save_predictions(test_tokens, test_true_labels, test_pred_labels, "final_test_predictions.csv")
ğŸ“‚ Expected CSV Output (Correctly Formatted)
Token	Actual_Tag	Predicted_Tag
Apple	B-ORG	B-ORG
is	O	O
a	O	O
company	O	B-ORG
in	O	O
USA	B-LOC	B-LOC
ğŸš€ Final Fixes Summary
âœ… Handles subwords correctly â†’ No "comp", "##uter" issues.
âœ… Ensures actual vs. predicted labels match words â†’ No missing tokens.
âœ… Properly saves train & test files in CSV format
################
To save the final output of train and test datasets with actual vs predicted tags, follow this complete approach. ğŸš€

ğŸ“Œ Steps
1ï¸âƒ£ Run Model Predictions on Train & Test Data
2ï¸âƒ£ Convert Predictions from IDs to Text Labels
3ï¸âƒ£ Align Words with Actual & Predicted Tags
4ï¸âƒ£ Save the Results in CSV Format

ğŸ”¹ Step 1: Generate Predictions
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    true_labels = [[id2label[label] for label in label_seq if label != -100] for label_seq in labels]
    pred_labels = [[id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100] 
                   for pred_seq, label_seq in zip(predictions, labels)]
    
    return true_labels, pred_labels
âœ… This removes padding tokens (-100) to keep only relevant words.

ğŸ”¹ Step 2: Save Predictions in CSV Format
python
Copy
Edit
import pandas as pd

def save_predictions(sentences, true_labels, pred_labels, filename):
    data = []
    for sentence, actual, predicted in zip(sentences, true_labels, pred_labels):
        for word, act_tag, pred_tag in zip(sentence, actual, predicted):
            data.append([word[0], act_tag, pred_tag])

    df = pd.DataFrame(data, columns=["Word", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"âœ… Saved: {filename}")

# ğŸ”¹ Predict & Save Train Data
train_true, train_pred = get_predictions(trainer, dataset_train)
save_predictions(dataset_train, train_true, train_pred, "final_train_predictions.csv")

# ğŸ”¹ Predict & Save Test Data
test_true, test_pred = get_predictions(trainer, dataset_test)
save_predictions(dataset_test, test_true, test_pred, "final_test_predictions.csv")
âœ… The final output files will be saved as: 1ï¸âƒ£ final_train_predictions.csv
2ï¸âƒ£ final_test_predictions.csv

ğŸ“‚ Expected CSV Output Format
Word	Actual_Tag	Predicted_Tag
Elon	B-PER	B-PER
Musk	I-PER	I-PER
founded	O	O
SpaceX	B-ORG	B-ORG
in	O	O
2002	B-DATE	B-DATE
âœ… Summary
Extracted predictions for Train & Test sets
Converted label IDs into text labels
Saved results in CSV format for easy comparison
Now you can compare Actual vs. Predicted Tags in the final files. ğŸš€

Would you like to add more insights like error analysis? ğŸ”
