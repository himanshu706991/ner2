
 Issue: KeyError: word_ids in Dataset
This error occurs because word_ids is not available in your dataset. This happens when:
1Ô∏è‚É£ The tokenizer is not a "Fast" tokenizer (use_fast=True is required).
2Ô∏è‚É£ The dataset format is incompatible with word_ids().
3Ô∏è‚É£ The dataset structure is not aligned properly after tokenization.

‚úÖ Fix: Ensure Fast Tokenizer is Used
Modify your tokenizer initialization to force using a Fast Tokenizer:

python
Copy
Edit
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)  # ‚úÖ Ensure Fast Tokenizer
üîπ Why?
Only Fast Tokenizers support .word_ids(), which is required for word alignment.

‚úÖ Fix: Correct Tokenization to Include word_ids()
Modify the tokenization function to correctly map words and labels.

python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],  # Extract words
        truncation=True,
        padding=True,
        is_split_into_words=True,  # ‚úÖ Needed for `word_ids()`
        return_tensors="pt"
    )

    # Ensure word_ids() is available
    if not hasattr(tokenized_inputs, "word_ids"):
        raise ValueError("Error: The tokenizer does not support `word_ids`. Use a `FastTokenizer`.")

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # ‚úÖ Map tokens to words
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding and special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
‚úÖ Fixes:

Ensures word_ids() is available before using it.
Uses batch_index=i correctly (avoids missing word mappings).
‚úÖ Fix: Modify Prediction Function to Ensure word_ids() Exists
If your dataset still doesn‚Äôt have word_ids(), update the prediction function to verify its presence.

python
Copy
Edit
import numpy as np
import pandas as pd

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        # Ensure word_ids exists
        word_ids = dataset[i].get("word_ids", None)
        if word_ids is None:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")  # Get actual tag
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")  # Get predicted tag

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"‚úÖ Saved: {filename}")
‚úÖ Fixes:

Checks if word_ids is available before accessing it.
Ensures proper token-label alignment before saving CSV.
‚úÖ Final Steps: Generate CSV for Train & Test
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
üìÇ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
üöÄ Summary
‚úÖ Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
‚úÖ Checks if word_ids exists before calling it.
‚úÖ Handles subword alignment correctly (avoids extra/missing tokens).
‚úÖ Saves results in a structured CSV format for Train & Test data.

Try these fixes and let me know if the error is resolved! üöÄüî•

You said:
i am using fast tokenizer
ChatGPT said:
Since you‚Äôre using a Fast Tokenizer, but still getting KeyError: word_ids, let‚Äôs systematically debug and fix it.

üîç Issue Breakdown
Even with a Fast Tokenizer, word_ids() might still fail due to:
1Ô∏è‚É£ Incorrect Dataset Handling ‚Äì Some datasets don‚Äôt retain word_ids() correctly.
2Ô∏è‚É£ Misalignment After Tokenization ‚Äì Word IDs might not be properly mapped.
3Ô∏è‚É£ Dataset Format Differences ‚Äì The dataset might not store word_ids() correctly.

‚úÖ Fix 1: Check if word_ids() is Available
Before calling word_ids(), print debug info to confirm if it exists.

python
Copy
Edit
# Tokenize a sample sentence and check word_ids
sample_sentence = ["Elon", "Musk", "founded", "SpaceX", "in", "2002", "."]
tokenized_sample = tokenizer(sample_sentence, is_split_into_words=True, return_tensors="pt")

print("Tokenized Input IDs:", tokenized_sample["input_ids"])
print("Word IDs:", tokenized_sample.word_ids() if hasattr(tokenized_sample, "word_ids") else "‚ùå Not Available")
üîπ If word_ids() prints None, the tokenizer isn‚Äôt returning word mappings.
üîπ If ‚ùå Not Available appears, your dataset might not be storing word_ids().

‚úÖ Fix 2: Ensure word_ids() is Stored in Dataset
Modify dataset creation to explicitly store word_ids() in each sample.

üîπ Correct Tokenization & Dataset Creation
python
Copy
Edit
import torch

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    aligned_labels = []
    word_ids_list = []  # ‚úÖ Store word IDs separately
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        word_ids_list.append(word_ids)  # ‚úÖ Store word IDs

        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    tokenized_inputs["word_ids"] = word_ids_list  # ‚úÖ Ensure word_ids are stored
    return tokenized_inputs
‚úÖ Fixes:

Ensures word_ids() is stored in the dataset.
Prevents KeyError: word_ids by keeping mappings available.
‚úÖ Fix 3: Modify Prediction Function to Access word_ids Safely
Modify your prediction function to safely access word_ids().

python
Copy
Edit
import numpy as np
import pandas as pd

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)
    labels = predictions_output.label_ids

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        # ‚úÖ Safely check for `word_ids`
        if "word_ids" not in dataset[i]:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        word_ids = dataset[i]["word_ids"]  # ‚úÖ Access stored word IDs
        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"‚úÖ Saved: {filename}")
‚úÖ Fixes:

Ensures word_ids exists in dataset before accessing it.
Prevents KeyError: word_ids when processing predictions.
‚úÖ Final Steps: Generate CSV for Train & Test
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
üìÇ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
üöÄ Summary
‚úÖ Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
‚úÖ Stores word_ids inside dataset explicitly to avoid missing mappings.
‚úÖ Safely accesses word_ids during predictions to prevent crashes.
‚úÖ Saves results in structured CSV format for Train & Test data.

üîé Next Steps
Try these fixes and check if word_ids() is properly stored in your dataset.
Print debug information (print(dataset_train[0])) to verify data structure.
Run the CSV export function to check if final outputs are correct.
üöÄ Let me know if the issue is resolved! üî•


################################################################$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4444444
$$$Keys error: word_ids in both cases i am getting error
ChatGPT said:
üîç Issue: KeyError: word_ids in Predictions
The error occurs because word_ids() is not available in some tokenizers (especially when using a tokenizer that doesn‚Äôt support word alignment).

‚úÖ Fix 1: Ensure Correct Tokenizer Usage
Make sure your tokenizer supports word alignment by using BertTokenizerFast.

python
Copy
Edit
from transformers import AutoTokenizer

# Ensure using a fast tokenizer that supports word_ids()
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER", use_fast=True)
‚úÖ Why?
Only Fast Tokenizers (like BertTokenizerFast) support word_ids() for word alignment.

‚úÖ Fix 2: Use .word_ids(batch_index=i) Properly
Modify the tokenization & prediction functions without assuming word_ids() is always present.

üîπ Correct Tokenization Function
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    if not hasattr(tokenized_inputs, "word_ids"):
        raise ValueError("Error: The tokenizer does not support `word_ids`. Use a `FastTokenizer`.")

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # ‚úÖ Ensures correct batch handling
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(sentence_labels[word_idx], 0))  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subwords

            previous_word_idx = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
‚úÖ Fixes:

Ensures word_ids() exists before calling it.
Uses batch_index=i correctly (avoids missing word mappings).
‚úÖ Fix 3: Handle word_ids() in Prediction Function
Modify the prediction function to ensure word alignment.

üîπ Correct Prediction Function
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    output_list = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        
        # Ensure word_ids exists
        if not hasattr(dataset[i], "word_ids"):
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")
        
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        sentence_tokens = []
        sentence_true_labels = []
        sentence_pred_labels = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            sentence_tokens.append(tokens[j])
            sentence_true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))
            sentence_pred_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))

            previous_word_idx = word_idx

        output_list.append([sentence_tokens, sentence_true_labels, sentence_pred_labels])

    return output_list
‚úÖ Fixes:

Checks if word_ids exists before using it.
Ensures subwords don‚Äôt get duplicate labels.
Properly maps predictions back to words.
‚úÖ Final Steps
1Ô∏è‚É£ Get Train & Test Predictions
python
Copy
Edit
train_predictions_list = get_predictions(trainer, dataset_train, tokenizer)
test_predictions_list = get_predictions(trainer, dataset_test, tokenizer)
2Ô∏è‚É£ Print & Verify Outputs
python
Copy
Edit
print("Train Example:", train_predictions_list[:3])
print("\nTest Example:", test_predictions_list[:3])
3Ô∏è‚É£ Save to JSON (Optional)
python
Copy
Edit
import json

with open("train_predictions.json", "w") as f:
    json.dump(train_predictions_list, f, indent=4)

with open("test_predictions.json", "w") as f:
    json.dump(test_predictions_list, f, indent=4)

print("‚úÖ Saved train_predictions.json and test_predictions.json")
üéØ Summary
‚úÖ Fixes KeyError: word_ids by ensuring Fast Tokenizer (use_fast=True).
‚úÖ Checks if word_ids exists before calling it.
‚úÖ Handles subword alignment correctly (avoids extra/missing tokens).
‚úÖ Saves results in a clean list format or JSON file.

üöÄ Try these fixes & let me know if the error is resolved! üî•





You said:
ok this is for list formate , i also need in csv, tokenid,predtag,actual tag
ChatGPT said:
‚úÖ Saving Predictions in CSV Format (Token, Token ID, Actual Tag, Predicted Tag)
Since you need Token, Token ID, Actual Tag, and Predicted Tag stored in a CSV file, we'll modify our function accordingly.

üîπ Step 1: Modify Prediction Function for CSV
python
Copy
Edit
import pandas as pd
import numpy as np

def get_predictions_for_csv(trainer, dataset, tokenizer, filename):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    data = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)
        word_ids = dataset[i].get("word_ids", None)  # Get word mappings if available

        if word_ids is None:
            raise ValueError("Error: `word_ids` not found in dataset. Ensure correct tokenizer is used.")

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding

            token_id = input_ids[j]
            actual_tag = id2label.get(int(labels[i][word_idx]), "O")  # Get actual tag
            predicted_tag = id2label.get(int(predictions[i][word_idx]), "O")  # Get predicted tag

            data.append([tokens[j], token_id, actual_tag, predicted_tag])
            previous_word_idx = word_idx

    # Create DataFrame and Save CSV
    df = pd.DataFrame(data, columns=["Token", "Token_ID", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"‚úÖ Saved: {filename}")
üîπ Step 2: Generate CSV Files for Train & Test Data
python
Copy
Edit
get_predictions_for_csv(trainer, dataset_train, tokenizer, "final_train_predictions.csv")
get_predictions_for_csv(trainer, dataset_test, tokenizer, "final_test_predictions.csv")
üìÇ Expected CSV Output Format
Token	Token_ID	Actual_Tag	Predicted_Tag
Elon	1234	B-PER	B-PER
Musk	5678	I-PER	I-PER
founded	9101	O	O
SpaceX	1112	B-ORG	B-ORG
in	1314	O	O
2002	1516	B-DATE	B-DATE
üöÄ Summary
‚úÖ Fixes missing/misaligned tokens issue
‚úÖ Ensures Token, Token ID, Actual Tag, and Predicted Tag are aligned
‚úÖ Saves results in a structured CSV format for Train & Test data

Now you can compare actual vs predicted tags in CSV easily! üéØ Let me know if you need further refinements. üöÄüî•





###############################################################################################3
Yes! You can store the output in a list format, where each entry contains:
1Ô∏è‚É£ Tokens (Words)
2Ô∏è‚É£ Actual Tags (Ground Truth)
3Ô∏è‚É£ Predicted Tags (Model Output)

‚úÖ Storing Output in a List Format
Modify the function to return the output as a list of lists instead of a CSV file:

python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    output_list = []  # Store final output in list format

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()  # Convert token IDs to tokens
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        sentence_tokens = []
        sentence_true_labels = []
        sentence_pred_labels = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding
            
            sentence_tokens.append(tokens[j])  # Store token
            sentence_true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))  # Actual tag
            sentence_pred_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))  # Predicted tag

            previous_word_idx = word_idx

        # Store as a list: [tokens_list, actual_tags_list, predicted_tags_list]
        output_list.append([sentence_tokens, sentence_true_labels, sentence_pred_labels])

    return output_list
‚úÖ Get Train & Test Predictions in List Format
python
Copy
Edit
train_predictions_list = get_predictions(trainer, dataset_train, tokenizer)
test_predictions_list = get_predictions(trainer, dataset_test, tokenizer)

# Example: Check the first few sentences
print("Train Example:", train_predictions_list[:3])
print("\nTest Example:", test_predictions_list[:3])
üìå Example Output (List Format)
python
Copy
Edit
[
    [
        ['Elon', 'Musk', 'founded', 'SpaceX', 'in', '2002', '.'],  # Tokens
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O'],  # Actual Tags
        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-DATE', 'O']   # Predicted Tags
    ],
    [
        ['Google', 'is', 'a', 'tech', 'company', '.'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O'],
        ['B-ORG', 'O', 'O', 'O', 'O', 'O']
    ]
]
‚úÖ Each sentence is stored as a nested list with [Tokens, Actual Tags, Predicted Tags].

üöÄ Benefits of List Format
‚úÖ Easy to process in Python (can iterate over sentences easily).
‚úÖ No formatting issues like CSV (ensures all words align correctly).
‚úÖ Can be saved in JSON for better readability.
‚úÖ Save Output to JSON (Optional)
If you want to save this structured format to a file:

python
Copy
Edit
import json

with open("train_predictions.json", "w") as f:
    json.dump(train_predictions_list, f, indent=4)

with open("test_predictions.json", "w") as f:
    json.dump(test_predictions_list, f, indent=4)

print("‚úÖ Saved train_predictions.json and test_predictions.json")
This will store the structured format in JSON.

üéØ Summary
You can now store output in a structured list format:
[Tokens, Actual Tags, Predicted Tags]
No subword issues, padding issues, or misaligned tokens.
Optionally save the results in JSON for easy visualization.
Try this and let me know if you need further modifications! üöÄüî•


######

üîç Issue: Tokens Not Aligning Properly in Final CSV
If your final train/test CSV files have errors such as:
1Ô∏è‚É£ Subwords instead of full words (e.g., "comp", "##uter" instead of "computer")
2Ô∏è‚É£ Mismatched token counts (Extra/missing words)
3Ô∏è‚É£ Too many arguments error

‚úÖ Fix: Properly Align Tokens, Actual Tags, and Predicted Tags
Modify your tokenization & alignment process to handle subwords correctly.

üîπ Step 1: Adjust Tokenization & Alignment
python
Copy
Edit
def tokenize_and_align_labels(sentences, labels, tokenizer):
    tokenized_inputs = tokenizer(
        [[word for word in sentence] for sentence in sentences],
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors="pt"
    )

    aligned_labels = []
    for i, sentence_labels in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id[sentence_labels[word_idx]])  # Assign correct label
            else:
                label_ids.append(-100)  # Ignore subword tokens
                
            previous_word_idx = word_idx
        
        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = torch.tensor(aligned_labels)
    return tokenized_inputs
‚úÖ This ensures:

Only the first subword gets a label
Subwords (##word) don‚Äôt get duplicate labels
Special tokens ([CLS], [SEP], [PAD]) are ignored
üîπ Step 2: Generate Predictions & Align Them with Tokens
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset, tokenizer):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Actual label indices

    true_labels_list = []
    predicted_labels_list = []
    tokens_list = []

    for i in range(len(dataset)):
        input_ids = dataset[i]["input_ids"].tolist()  # Convert token IDs to tokens
        word_ids = dataset[i]["word_ids"]
        tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        true_labels = []
        predicted_labels = []
        aligned_tokens = []

        previous_word_idx = None
        for j, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                continue  # Ignore subwords and padding
            
            aligned_tokens.append(tokens[j])  # Get full words
            true_labels.append(id2label.get(int(labels[i][word_idx]), "O"))  # Assign correct actual label
            predicted_labels.append(id2label.get(int(predictions[i][word_idx]), "O"))  # Assign predicted label

            previous_word_idx = word_idx

        tokens_list.append(aligned_tokens)
        true_labels_list.append(true_labels)
        predicted_labels_list.append(predicted_labels)

    return tokens_list, true_labels_list, predicted_labels_list
‚úÖ This ensures:

Subword tokens don‚Äôt appear separately
Actual & predicted labels match words correctly
üîπ Step 3: Save Train & Test Predictions to CSV
python
Copy
Edit
import pandas as pd

def save_predictions(tokens, true_labels, pred_labels, filename):
    flat_tokens = [word for sentence in tokens for word in sentence]
    flat_actual_labels = [label for sentence in true_labels for label in sentence]
    flat_predicted_labels = [label for sentence in pred_labels for label in sentence]

    df = pd.DataFrame({"Token": flat_tokens, "Actual_Tag": flat_actual_labels, "Predicted_Tag": flat_predicted_labels})
    df.to_csv(filename, index=False)
    print(f"‚úÖ Saved: {filename}")

# üîπ Get predictions for Train & Test
train_tokens, train_true_labels, train_pred_labels = get_predictions(trainer, dataset_train, tokenizer)
test_tokens, test_true_labels, test_pred_labels = get_predictions(trainer, dataset_test, tokenizer)

# üîπ Save to CSV
save_predictions(train_tokens, train_true_labels, train_pred_labels, "final_train_predictions.csv")
save_predictions(test_tokens, test_true_labels, test_pred_labels, "final_test_predictions.csv")
üìÇ Expected CSV Output (Correctly Formatted)
Token	Actual_Tag	Predicted_Tag
Apple	B-ORG	B-ORG
is	O	O
a	O	O
company	O	B-ORG
in	O	O
USA	B-LOC	B-LOC
üöÄ Final Fixes Summary
‚úÖ Handles subwords correctly ‚Üí No "comp", "##uter" issues.
‚úÖ Ensures actual vs. predicted labels match words ‚Üí No missing tokens.
‚úÖ Properly saves train & test files in CSV format
################
To save the final output of train and test datasets with actual vs predicted tags, follow this complete approach. üöÄ

üìå Steps
1Ô∏è‚É£ Run Model Predictions on Train & Test Data
2Ô∏è‚É£ Convert Predictions from IDs to Text Labels
3Ô∏è‚É£ Align Words with Actual & Predicted Tags
4Ô∏è‚É£ Save the Results in CSV Format

üîπ Step 1: Generate Predictions
python
Copy
Edit
import numpy as np

def get_predictions(trainer, dataset):
    predictions_output = trainer.predict(dataset)
    predictions = np.argmax(predictions_output.predictions, axis=2)  # Convert logits to label indices
    labels = predictions_output.label_ids  # Get actual label indices

    true_labels = [[id2label[label] for label in label_seq if label != -100] for label_seq in labels]
    pred_labels = [[id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100] 
                   for pred_seq, label_seq in zip(predictions, labels)]
    
    return true_labels, pred_labels
‚úÖ This removes padding tokens (-100) to keep only relevant words.

üîπ Step 2: Save Predictions in CSV Format
python
Copy
Edit
import pandas as pd

def save_predictions(sentences, true_labels, pred_labels, filename):
    data = []
    for sentence, actual, predicted in zip(sentences, true_labels, pred_labels):
        for word, act_tag, pred_tag in zip(sentence, actual, predicted):
            data.append([word[0], act_tag, pred_tag])

    df = pd.DataFrame(data, columns=["Word", "Actual_Tag", "Predicted_Tag"])
    df.to_csv(filename, index=False)
    print(f"‚úÖ Saved: {filename}")

# üîπ Predict & Save Train Data
train_true, train_pred = get_predictions(trainer, dataset_train)
save_predictions(dataset_train, train_true, train_pred, "final_train_predictions.csv")

# üîπ Predict & Save Test Data
test_true, test_pred = get_predictions(trainer, dataset_test)
save_predictions(dataset_test, test_true, test_pred, "final_test_predictions.csv")
‚úÖ The final output files will be saved as: 1Ô∏è‚É£ final_train_predictions.csv
2Ô∏è‚É£ final_test_predictions.csv

üìÇ Expected CSV Output Format
Word	Actual_Tag	Predicted_Tag
Elon	B-PER	B-PER
Musk	I-PER	I-PER
founded	O	O
SpaceX	B-ORG	B-ORG
in	O	O
2002	B-DATE	B-DATE
‚úÖ Summary
Extracted predictions for Train & Test sets
Converted label IDs into text labels
Saved results in CSV format for easy comparison
Now you can compare Actual vs. Predicted Tags in the final files. üöÄ

Would you like to add more insights like error analysis? üîç
